{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f28141b",
   "metadata": {},
   "source": [
    "# Root Cause Analysis - PyRCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc2774",
   "metadata": {},
   "source": [
    "## Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pyrca\n",
    "\n",
    "from pyrca.analyzers.ht import HT, HTConfig\n",
    "from pyrca.analyzers.epsilon_diagnosis import EpsilonDiagnosis, EpsilonDiagnosisConfig\n",
    "from pyrca.analyzers.bayesian import BayesianNetwork, BayesianNetworkConfig\n",
    "from pyrca.analyzers.random_walk import RandomWalk, RandomWalkConfig\n",
    "from pyrca.analyzers.rcd import RCD, RCDConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303a36a",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0bed2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_save(G,pos,node_colors,file_name:str, inter_type:str):\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.xlim((-12,14))\n",
    "    plt.ylim((-10,8))\n",
    "    plt.title(f'Causal Graph: {inter_type}', fontsize=15)\n",
    "    nx.draw(G, pos,with_labels=True,node_size=2000, node_color=[node_colors[node] for node in G.nodes()], font_size=8, arrowsize=10,width=0.5)\n",
    "    plt.savefig(file_name)\n",
    "\n",
    "def get_file_name(var:str):\n",
    "    return var[7:-4]\n",
    "\n",
    "def create_data_from_list(files):\n",
    "    data_df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        to_join_df = file\n",
    "        data_df = pd.concat([data_df,to_join_df.loc[to_join_df.index]],ignore_index=True)\n",
    "    return data_df\n",
    "\n",
    "def create_train_data(files,startrow):\n",
    "    data_df = pd.DataFrame()\n",
    "    for file_key, file_path in files.items():\n",
    "        to_join_df =  pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "        data_df = pd.concat([data_df,to_join_df.loc[to_join_df.index]],ignore_index=True)\n",
    "    return data_df\n",
    "\n",
    "def get_from_dir(directory_path):\n",
    "    # Get a list of all items (files and directories) in the specified path\n",
    "    all_items = os.listdir(directory_path)\n",
    "    # Iterate over each item and check if it's a directory\n",
    "    for item in all_items:\n",
    "        folder_path = os.path.join(directory_path, item)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"\\nProcessing folder: {item}\")\n",
    "            folder_contents = os.listdir(folder_path)\n",
    "            for file in folder_contents:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                print(f\"    -> {file}\")\n",
    "\n",
    "def get_from_folder(directory_path,folder):\n",
    "    files = {}\n",
    "    folder_path = os.path.join(directory_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Selected folder: {folder}\")\n",
    "        folder_contents = os.listdir(folder_path)\n",
    "        for file in folder_contents:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            print(f\"-> {file}\")\n",
    "            if file[0:4] == 'data':\n",
    "                files[f\"{get_file_name(file)}\"] = file_path\n",
    "    return files, folder_path\n",
    "\n",
    "# Function run hypothesis testing algorithm\n",
    "def run_HT(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           G: nx.DiGraph,\n",
    "           nodes: list,\n",
    "           edges:dict,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "     \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)  # Make sure `edges` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G, nodes)\n",
    "    \n",
    "    interventions = ['gripper_1', 'gripper_2', 'max_Vel_1', 'max_Vel_2', 'camera_1', 'camera_2', 'camera_3',\n",
    "                     'conveyor_1', 'conveyor_2', 'conveyor_3']\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "\n",
    "    if 'normal' not in files:\n",
    "        print('No normal data file found in folder!!')\n",
    "        return\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        if file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            draw_save(G, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if file_key in interventions:\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key)\n",
    "    \n",
    "    if not file_names:\n",
    "        print('No abnormal data in folder found!!')\n",
    "        return\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    model = HT(config=HTConfig(adj_matrix_extended_pd))\n",
    "    model.train(normal_data_df)\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            if (abnormal_data_df[node] == 0.0).any():\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "                results[node] = model.find_root_causes(abnormal_data_df, node, True).to_list()\n",
    "        \n",
    "        results_file_name = f'results_{file_names[file_counter]}_HT.csv'\n",
    "        results.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for node in abnormal_nodes:\n",
    "            rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "            rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "            rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_HT.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        draw_save(G, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "# Function run epsilon diagnosis algorithm\n",
    "def run_ED(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           G: nx.DiGraph,\n",
    "           nodes: list,\n",
    "           edges:dict,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "     \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)  # Make sure `edges` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G, nodes)\n",
    "    \n",
    "    interventions = ['gripper_1', 'gripper_2', 'max_Vel_1', 'max_Vel_2', 'camera_1', 'camera_2', 'camera_3',\n",
    "                     'conveyor_1', 'conveyor_2', 'conveyor_3']\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "\n",
    "    if 'normal' not in files:\n",
    "        print('No normal data file found in folder!!')\n",
    "        return\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        if file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            draw_save(G, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if file_key in interventions:\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key)\n",
    "    \n",
    "    if not file_names:\n",
    "        print('No abnormal data in folder found!!')\n",
    "        return\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    model = EpsilonDiagnosis(EpsilonDiagnosisConfig(alpha=0.05))\n",
    "    model.train(normal_data_df)\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] == 0.0).any():\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_data_df)\n",
    "\n",
    "        results_file_name = f'results_{file_names[file_counter]}_ED.csv'\n",
    "        \n",
    "        root_cause_nodes = pd.DataFrame(results.root_cause_nodes)\n",
    "        \n",
    "        root_cause_nodes.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        new_colors[root_cause_nodes[0][0]] = 'red'\n",
    "\n",
    "        new_colors[root_cause_nodes[0][1]] = 'crimson'\n",
    "\n",
    "        new_colors[root_cause_nodes[0][2]] = 'indianred'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_ED.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        draw_save(G, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "# Function run random walk algorithm\n",
    "def run_RW(folder_path: str,\n",
    "           files: dict,\n",
    "           train_file:pd.DataFrame,\n",
    "           startrow: int,\n",
    "           G: nx.DiGraph,\n",
    "           nodes: list,\n",
    "           edges:dict,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "     \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)  # Make sure `edges` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G.nodes(), columns=G.nodes())\n",
    "    \n",
    "    interventions = ['gripper_1', 'gripper_2', 'max_Vel_1', 'max_Vel_2', 'camera_1', 'camera_2', 'camera_3',\n",
    "                     'conveyor_1', 'conveyor_2', 'conveyor_3']\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "\n",
    "    if 'normal' not in files:\n",
    "        print('No normal data file found in folder!!')\n",
    "        return\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        if file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_RW.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            draw_save(G, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if file_key in interventions:\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key)\n",
    "    \n",
    "    if not file_names:\n",
    "        print('No abnormal data in folder found!!')\n",
    "        return\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    model = RandomWalk(RandomWalkConfig(graph=adjacency_df,root_cause_top_k=3))\n",
    "\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] == 0.0).any():\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_nodes,create_data_from_list([train_file,abnormal_data_df,normal_data_df]))\n",
    "\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RW.csv'\n",
    "        \n",
    "        root_cause_nodes = pd.DataFrame(results.root_cause_nodes)\n",
    "        \n",
    "        root_cause_nodes.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        new_colors[root_cause_nodes[0][0]] = 'red'\n",
    "\n",
    "        new_colors[root_cause_nodes[0][1]] = 'crimson'\n",
    "\n",
    "        new_colors[root_cause_nodes[0][2]] = 'indianred'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_RW.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        draw_save(G, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "# Function run Root cause discovery algorithm\n",
    "def run_RCD(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           G: nx.DiGraph,\n",
    "           nodes: list,\n",
    "           edges:dict,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "     \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)  # Make sure `edges` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G.nodes(), columns=G.nodes())\n",
    "    \n",
    "    interventions = ['gripper_1', 'gripper_2', 'max_Vel_1', 'max_Vel_2', 'camera_1', 'camera_2', 'camera_3',\n",
    "                     'conveyor_1', 'conveyor_2', 'conveyor_3']\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "\n",
    "    if 'normal' not in files:\n",
    "        print('No normal data file found in folder!!')\n",
    "        return\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        if file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_RW.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            draw_save(G, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if file_key in interventions:\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key)\n",
    "    \n",
    "    if not file_names:\n",
    "        print('No abnormal data in folder found!!')\n",
    "        return\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    model = RCD(RCDConfig())\n",
    "    \n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            if (abnormal_data_df[node] == 0.0).any():\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "                results[node] = model.find_root_causes(normal_data_df,abnormal_data_df).to_list()\n",
    "        \n",
    "        print(results)\n",
    "        \n",
    "        # results_file_name = f'results_{file_names[file_counter]}_HT.csv'\n",
    "        # results.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "        \n",
    "        # rank1_root_cause = []\n",
    "        # rank2_root_cause = []\n",
    "        # rank3_root_cause = []\n",
    "\n",
    "        # for node in abnormal_nodes:\n",
    "        #     rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "        #     rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "        #     rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "        # for node in rank1_root_cause:\n",
    "        #     new_colors[node] = 'red'\n",
    "\n",
    "        # for node in rank2_root_cause:\n",
    "        #     new_colors[node] = 'crimson'\n",
    "\n",
    "        # for node in rank3_root_cause:\n",
    "        #     new_colors[node] = 'lightcoral'\n",
    "\n",
    "        # filename = f'{file_names[file_counter]}_HT.png'\n",
    "\n",
    "        # file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        # draw_save(G, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "# Function run bayesian network algorithm\n",
    "def run_BN(folder_path: str,\n",
    "           files: dict,\n",
    "           train_file:pd.DataFrame,\n",
    "           startrow: int,\n",
    "           G: nx.DiGraph,\n",
    "           nodes: list,\n",
    "           edges:dict,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "     \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)  # Make sure `edges` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G.nodes(), columns=G.nodes())\n",
    "    \n",
    "    interventions = ['gripper_1', 'gripper_2', 'max_Vel_1', 'max_Vel_2', 'camera_1', 'camera_2', 'camera_3',\n",
    "                     'conveyor_1', 'conveyor_2', 'conveyor_3']\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "\n",
    "    if 'normal' not in files:\n",
    "        print('No normal data file found in folder!!')\n",
    "        return\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        if file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_RW.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            draw_save(G, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if file_key in interventions:\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key)\n",
    "    \n",
    "    if not file_names:\n",
    "        print('No abnormal data in folder found!!')\n",
    "        return\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    model = BayesianNetwork(BayesianNetworkConfig(adjacency_df,thres_reduce_func=\"median\",root_cause_top_k=3))\n",
    "\n",
    "    abnormal_train_data = create_train_data(abnormal_files,startrow)\n",
    "    \n",
    "    model.train(create_data_from_list([train_file,abnormal_train_data,normal_data_df]))\n",
    "\n",
    "    # for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "    #     abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "    #     abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "    #     abnormal_nodes = []\n",
    "    #     new_colors = colors.copy()\n",
    "\n",
    "    #     for node in key_nodes:\n",
    "    #         # Basic Anomaly Detection\n",
    "    #         if (abnormal_data_df[node] == 0.0).any():\n",
    "    #             abnormal_nodes.append(node)\n",
    "    #             new_colors[node] = 'yellow'\n",
    "\n",
    "    #     results = model.find_root_causes(abnormal_nodes)\n",
    "\n",
    "    #     print(results)\n",
    "\n",
    "        # results_file_name = f'results_{file_names[file_counter]}_RW.csv'\n",
    "        \n",
    "        # root_cause_nodes = pd.DataFrame(results.root_cause_nodes)\n",
    "        \n",
    "        # root_cause_nodes.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        # new_colors[root_cause_nodes[0][0]] = 'red'\n",
    "\n",
    "        # new_colors[root_cause_nodes[0][1]] = 'crimson'\n",
    "\n",
    "        # new_colors[root_cause_nodes[0][2]] = 'indianred'\n",
    "\n",
    "        # filename = f'{file_names[file_counter]}_RW.png'\n",
    "\n",
    "        # file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        # draw_save(G, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2adbc",
   "metadata": {},
   "source": [
    "## Create the Causal Graph (non-lagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb8781",
   "metadata": {},
   "source": [
    "### Graph Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ddf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "nodes = [\n",
    "    'cam_1_X', 'cam_2_X', 'cam_3_X',\n",
    "    'cam_1_Y', 'cam_2_Y', 'cam_3_Y',\n",
    "    'EoL_1_X', 'EoL_2_X', 'EoL_3_X', 'EoL_4_X', 'EoL_5_X', 'EoL_6_X',\n",
    "    'EoL_1_Y', 'EoL_2_Y', 'EoL_3_Y', 'EoL_4_Y', 'EoL_5_Y', 'EoL_6_Y',\n",
    "    'rob_1_1', 'rob_1_2', 'rob_1_3', 'rob_1_4', 'rob_1_maxVel',\n",
    "    'rob_2_1', 'rob_2_2', 'rob_2_3', 'rob_2_4', 'rob_2_maxVel',\n",
    "    'rob_1_gripper', 'rob_2_gripper',\n",
    "    'con_1','con_2','con_3'\n",
    "]\n",
    "\n",
    "edges = [\n",
    "    ('cam_1_X', 'rob_2_1'), ('cam_1_Y', 'rob_2_1'),\n",
    "    ('cam_1_X', 'rob_2_2'), ('cam_1_Y', 'rob_2_2'),\n",
    "    ('cam_1_X', 'rob_2_3'), ('cam_1_Y', 'rob_2_3'),\n",
    "    ('cam_1_X', 'rob_2_4'), ('cam_1_Y', 'rob_2_4'),\n",
    "    \n",
    "    ('cam_2_X', 'rob_1_1'), ('cam_2_Y', 'rob_1_1'),\n",
    "    ('cam_2_X', 'rob_1_2'), ('cam_2_Y', 'rob_1_2'),\n",
    "    ('cam_2_X', 'rob_1_3'), ('cam_2_Y', 'rob_1_3'),\n",
    "    ('cam_2_X', 'rob_1_4'), ('cam_2_Y', 'rob_1_4'),\n",
    "    \n",
    "    ('cam_3_X', 'rob_1_1'), ('cam_3_Y', 'rob_1_1'),\n",
    "    ('cam_3_X', 'rob_1_2'), ('cam_3_Y', 'rob_1_2'),\n",
    "    ('cam_3_X', 'rob_1_3'), ('cam_3_Y', 'rob_1_3'),\n",
    "    ('cam_3_X', 'rob_1_4'), ('cam_3_Y', 'rob_1_4'),\n",
    "    \n",
    "    ('rob_1_maxVel', 'rob_1_1'), ('rob_1_maxVel', 'rob_1_2'),\n",
    "    ('rob_1_maxVel', 'rob_1_3'), ('rob_1_maxVel', 'rob_1_4'),\n",
    "    \n",
    "    ('rob_2_maxVel', 'rob_2_1'), ('rob_2_maxVel', 'rob_2_2'),\n",
    "    ('rob_2_maxVel', 'rob_2_3'), ('rob_2_maxVel', 'rob_2_4'),\n",
    "    \n",
    "    ('con_2', 'rob_1_1'), ('con_2', 'rob_1_2'), ('con_2', 'rob_1_3'), ('con_2', 'rob_1_4'),\n",
    "    ('con_3', 'rob_1_1'), ('con_3', 'rob_1_2'), ('con_3', 'rob_1_3'), ('con_3', 'rob_1_4'),\n",
    "\n",
    "    ('con_2', 'rob_2_1'), ('con_2', 'rob_2_2'), ('con_2', 'rob_2_3'), ('con_2', 'rob_2_4'),\n",
    "    ('con_1', 'rob_2_1'), ('con_1', 'rob_2_2'), ('con_1', 'rob_2_3'), ('con_1', 'rob_2_4'),\n",
    "\n",
    "    ('con_2', 'EoL_1_X'), ('con_2', 'EoL_1_Y'),\n",
    "    \n",
    "    ('rob_1_1', 'rob_2_1'), ('rob_1_1', 'rob_2_2'), ('rob_1_1', 'rob_2_3'), ('rob_1_1', 'rob_2_4'),\n",
    "    ('rob_1_2', 'rob_2_1'), ('rob_1_2', 'rob_2_2'), ('rob_1_2', 'rob_2_3'), ('rob_1_2', 'rob_2_4'),\n",
    "    ('rob_1_3', 'rob_2_1'), ('rob_1_3', 'rob_2_2'), ('rob_1_3', 'rob_2_3'), ('rob_1_3', 'rob_2_4'),\n",
    "    ('rob_1_4', 'rob_2_1'), ('rob_1_4', 'rob_2_2'), ('rob_1_4', 'rob_2_3'), ('rob_1_4', 'rob_2_4'),\n",
    "    \n",
    "    ('rob_1_gripper', 'rob_2_1'), ('rob_1_gripper', 'rob_2_2'),\n",
    "    ('rob_1_gripper', 'rob_2_3'), ('rob_1_gripper', 'rob_2_4'),\n",
    "\n",
    "    ('rob_1_1', 'EoL_2_X'), ('rob_1_2', 'EoL_2_X'),\n",
    "    ('rob_1_3', 'EoL_2_X'), ('rob_1_4', 'EoL_2_X'),\n",
    "    ('rob_1_1', 'EoL_2_Y'), ('rob_1_2', 'EoL_2_Y'),\n",
    "    ('rob_1_3', 'EoL_2_Y'), ('rob_1_4', 'EoL_2_Y'),\n",
    "    \n",
    "    ('rob_2_1', 'EoL_3_X'), ('rob_2_2', 'EoL_3_X'),\n",
    "    ('rob_2_3', 'EoL_3_X'), ('rob_2_4', 'EoL_3_X'),\n",
    "    ('rob_2_1', 'EoL_3_Y'), ('rob_2_2', 'EoL_3_Y'),\n",
    "    ('rob_2_3', 'EoL_3_Y'), ('rob_2_4', 'EoL_3_Y'),\n",
    "    \n",
    "    ('rob_2_1', 'EoL_4_X'), ('rob_2_2', 'EoL_4_X'),\n",
    "    ('rob_2_3', 'EoL_4_X'), ('rob_2_4', 'EoL_4_X'),\n",
    "    ('rob_2_1', 'EoL_4_Y'), ('rob_2_2', 'EoL_4_Y'),\n",
    "    ('rob_2_3', 'EoL_4_Y'), ('rob_2_4', 'EoL_4_Y'),\n",
    "    \n",
    "    ('rob_2_1', 'EoL_5_X'), ('rob_2_2', 'EoL_5_X'),\n",
    "    ('rob_2_3', 'EoL_5_X'), ('rob_2_4', 'EoL_5_X'),\n",
    "    ('rob_2_1', 'EoL_5_Y'), ('rob_2_2', 'EoL_5_Y'),\n",
    "    ('rob_2_3', 'EoL_5_Y'), ('rob_2_4', 'EoL_5_Y'),\n",
    "\n",
    "    ('rob_2_1', 'EoL_6_X'), ('rob_2_2', 'EoL_6_X'),\n",
    "    ('rob_2_3', 'EoL_6_X'), ('rob_2_4', 'EoL_6_X'),\n",
    "    ('rob_2_1', 'EoL_6_Y'), ('rob_2_2', 'EoL_6_Y'),\n",
    "    ('rob_2_3', 'EoL_6_Y'), ('rob_2_4', 'EoL_6_Y'),\n",
    "\n",
    "    ('rob_1_gripper', 'EoL_2_X'), ('rob_1_gripper', 'EoL_2_Y'),\n",
    "    \n",
    "    ('rob_2_gripper', 'EoL_3_X'), ('rob_2_gripper', 'EoL_3_Y'),\n",
    "    ('rob_2_gripper', 'EoL_4_X'), ('rob_2_gripper', 'EoL_4_Y'),\n",
    "    ('rob_2_gripper', 'EoL_5_X'), ('rob_2_gripper', 'EoL_5_Y'),\n",
    "    ('rob_2_gripper', 'EoL_6_X'), ('rob_2_gripper', 'EoL_6_Y'),\n",
    "]\n",
    "\n",
    "EoL_nodes = [\n",
    "    'EoL_1_X', 'EoL_2_X', 'EoL_3_X', 'EoL_4_X', 'EoL_5_X', 'EoL_6_X',\n",
    "    'EoL_1_Y', 'EoL_2_Y', 'EoL_3_Y', 'EoL_4_Y', 'EoL_5_Y', 'EoL_6_Y'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6da23",
   "metadata": {},
   "source": [
    "### Additional Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {\n",
    "    'cam_1_X':(7,4), 'cam_2_X':(-9,6), 'cam_3_X':(-5,6),\n",
    "    'cam_1_Y':(7,2), 'cam_2_Y':(-7,6), 'cam_3_Y':(-3,6),\n",
    "    'EoL_1_X':(10,-8), 'EoL_2_X':(-10,-8), 'EoL_3_X':(-6,-8), 'EoL_4_X':(-2,-8), 'EoL_5_X':(2,-8), 'EoL_6_X':(6,-8),\n",
    "    'EoL_1_Y':(12,-8), 'EoL_2_Y':(-8,-8), 'EoL_3_Y':(-4,-8), 'EoL_4_Y':(0,-8), 'EoL_5_Y':(4,-8), 'EoL_6_Y':(8,-8),\n",
    "    'rob_2_1':(-6,-4), 'rob_2_2':(-4,-4), 'rob_2_3':(-2,-4), 'rob_2_4':(-0,-4), 'rob_2_maxVel':(2,-4),\n",
    "    'rob_1_1':(-9,1), 'rob_1_2':(-7,1), 'rob_1_3':(-5,1), 'rob_1_4':(-3,1), 'rob_1_maxVel':(-1,1),\n",
    "    'rob_1_gripper':(1,1), 'rob_2_gripper':(4,-4),\n",
    "    'con_1':(7,-1),'con_2':(7,6),'con_3':(3,6)\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'cam_1_X':'skyblue', 'cam_2_X':'skyblue', 'cam_3_X':'skyblue',\n",
    "    'cam_1_Y':'skyblue', 'cam_2_Y':'skyblue', 'cam_3_Y':'skyblue',\n",
    "    'EoL_1_X':'lightgreen', 'EoL_2_X':'lightgreen', 'EoL_3_X':'lightgreen', 'EoL_4_X':'lightgreen', 'EoL_5_X':'lightgreen', 'EoL_6_X':'lightgreen',\n",
    "    'EoL_1_Y':'lightgreen', 'EoL_2_Y':'lightgreen', 'EoL_3_Y':'lightgreen', 'EoL_4_Y':'lightgreen', 'EoL_5_Y':'lightgreen', 'EoL_6_Y':'lightgreen',\n",
    "    'rob_1_1':'tan', 'rob_1_2':'tan', 'rob_1_3':'tan', 'rob_1_4':'tan', 'rob_1_maxVel':'tan',\n",
    "    'rob_2_1':'tan', 'rob_2_2':'tan', 'rob_2_3':'tan', 'rob_2_4':'tan', 'rob_2_maxVel':'tan',\n",
    "    'rob_1_gripper':'tan', 'rob_2_gripper':'tan',\n",
    "    'con_1':'lightgrey','con_2':'lightgrey','con_3':'lightgrey'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c55d6",
   "metadata": {},
   "source": [
    "## Show datasets in folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "directory_path = 'G:\\\\My Drive\\\\Master Thesis\\\\Simulation\\\\Dataset'\n",
    "get_from_dir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_input = input(\"Please select dataset folder.\")\n",
    "files,folder_path = get_from_folder(directory_path,folder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12b205",
   "metadata": {},
   "source": [
    "## Select folder for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = input(\"Please select train folder.\")\n",
    "train_files,train_folder_path = get_from_folder(directory_path,train_folder)\n",
    "train_file_df = create_train_data(train_files,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa24f2",
   "metadata": {},
   "source": [
    "## Run RCA trough folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c8120",
   "metadata": {},
   "source": [
    "### Algorithm #1 - Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_HT(folder_path,files,startrow=1000,G=G,nodes=nodes,edges=edges,key_nodes=EoL_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663bf75",
   "metadata": {},
   "source": [
    "### Algorithm #2 - Epsilon Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbec1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ED(folder_path,files,startrow=1000,G=G,nodes=nodes,edges=edges,key_nodes=EoL_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a7998",
   "metadata": {},
   "source": [
    "### Algorithm #3 - Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_RW(folder_path,files,train_file_df,startrow=1000,G=G,nodes=nodes,edges=edges,key_nodes=EoL_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0bb38f",
   "metadata": {},
   "source": [
    "### Algorithm #4: Bayesian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d4d94",
   "metadata": {},
   "source": [
    "run_BN(folder_path,files,train_file_df,startrow=1000,G=G,nodes=nodes,edges=edges,key_nodes=EoL_nodes,colors=colors,pos=pos)\n",
    "\n",
    "#Error: The cardinality of the variable con_2 should be = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294504a",
   "metadata": {},
   "source": [
    "### Algorithm #5: RCD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401cf78d",
   "metadata": {},
   "source": [
    "run_RCD(folder_path,files,startrow=1000,G=G,nodes=nodes,edges=edges,key_nodes=EoL_nodes,colors=colors,pos=pos)\n",
    "\n",
    "#Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71073591",
   "metadata": {},
   "source": [
    "### Algorithm #6: BARO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd47ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
