{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f28141b",
   "metadata": {},
   "source": [
    "# Root Cause Analysis - PyRCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc2774",
   "metadata": {},
   "source": [
    "## Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0ac0df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pyrca\n",
    "\n",
    "from pyrca.analyzers.ht import HT, HTConfig\n",
    "from pyrca.analyzers.epsilon_diagnosis import EpsilonDiagnosis, EpsilonDiagnosisConfig\n",
    "from pyrca.analyzers.bayesian import BayesianNetwork, BayesianNetworkConfig\n",
    "from pyrca.analyzers.random_walk import RandomWalk, RandomWalkConfig\n",
    "from pyrca.analyzers.rcd import RCD, RCDConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Some functions and libraries throw warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303a36a",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c164508",
   "metadata": {},
   "source": [
    "### Data and Directory Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5e006e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_save(G_graph,pos,node_colors,file_name:str, inter_type:str):\n",
    "    test = 'test' # no drawings\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.xlim((-12,14))\n",
    "    plt.ylim((-12,8))\n",
    "    plt.title(f'Causal Graph: {inter_type}', fontsize=12)\n",
    "    nx.draw(G_graph, pos,with_labels=True,node_size=2000, node_color=[node_colors[node] for node in G_graph.nodes()], font_size=6, arrowsize=8,width=0.5)\n",
    "    # plt.savefig(file_name)\n",
    "    # nx.write_gml(G_graph, f'{file_name[:-4]}.gml')\n",
    "\n",
    "def get_file_name(var:str):\n",
    "    return var[7:-4]\n",
    "\n",
    "def create_data_from_list(files):\n",
    "    data_df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        to_join_df = file\n",
    "        data_df = pd.concat([data_df,to_join_df.loc[to_join_df.index]],ignore_index=True)\n",
    "    return data_df\n",
    "\n",
    "def create_train_data(files,startrow):\n",
    "    data_df = pd.DataFrame()\n",
    "    for file_key, file_path in files.items():\n",
    "        to_join_df =  pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "        data_df = pd.concat([data_df,to_join_df.loc[to_join_df.index]],ignore_index=True)\n",
    "    return data_df\n",
    "\n",
    "def get_from_dir(directory_path):\n",
    "    # Get a list of all items (files and directories) in the specified path\n",
    "    all_items = os.listdir(directory_path)\n",
    "    # Iterate over each item and check if it's a directory\n",
    "    for item in all_items:\n",
    "        folder_path = os.path.join(directory_path, item)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"\\nProcessing folder: {item}\")\n",
    "            folder_contents = os.listdir(folder_path)\n",
    "            for file in folder_contents:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                print(f\"    -> {file}\")\n",
    "\n",
    "def get_from_folders(directory_path):\n",
    "    files = {}\n",
    "    all_folders = os.listdir(directory_path)\n",
    "    folder_counter = 0\n",
    "    for folder in all_folders:\n",
    "        folder_path = os.path.join(directory_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            folder_contents = os.listdir(folder_path)\n",
    "            folder_counter +=1\n",
    "            file_counter = 0\n",
    "            for file in folder_contents:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                if file[0:4] == 'data' and file[7:12] != 'event':\n",
    "                    file_counter += 1\n",
    "                    files[f\"{get_file_name(file)}-{folder_counter}.{file_counter}\"] = file_path\n",
    "    return files, directory_path\n",
    "\n",
    "def results_top_3(results,abnormal_set):\n",
    "    result_columns = results.columns.to_list()\n",
    "    result_index = results.index.to_list()\n",
    "\n",
    "    result_data_3top = pd.DataFrame(columns=result_index, index=result_columns)\n",
    "\n",
    "    for col in result_columns:\n",
    "        for ind in result_index:\n",
    "            for elem in results[col][ind]:\n",
    "                if elem in abnormal_set[col.rsplit('-', 1)[0]].to_list():\n",
    "                    result_data_3top[ind][col] = 1\n",
    "\n",
    "    result_data_3top = result_data_3top.fillna(0)\n",
    "\n",
    "    total_hit = (result_data_3top.sum('index')/len(result_columns)).to_dict()\n",
    "    result_data_3top = pd.concat([result_data_3top,pd.DataFrame(total_hit,index=['Total'])])\n",
    "\n",
    "    return result_data_3top\n",
    "\n",
    "def results_top_1(results,abnormal_set):\n",
    "    result_columns = results.columns.to_list()\n",
    "    result_index = results.index.to_list()\n",
    "    result_data_1top = pd.DataFrame(columns=result_index, index=result_columns)\n",
    "    for col in result_columns:\n",
    "        for ind in result_index:\n",
    "            try: elem = results[col][ind][0] # In case where RCD did not provide any root causes\n",
    "            except: continue\n",
    "            if elem in abnormal_set[col.rsplit('-', 1)[0]].to_list():\n",
    "                result_data_1top[ind][col] = 1\n",
    "\n",
    "    result_data_1top = result_data_1top.fillna(0)\n",
    "\n",
    "    total_hit = (result_data_1top.sum('index')/len(result_columns)).to_dict()\n",
    "    result_data_1top = pd.concat([result_data_1top,pd.DataFrame(total_hit,index=['Total'])])\n",
    "\n",
    "    return result_data_1top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf1fc2d",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e576a92",
   "metadata": {},
   "source": [
    "#### HT - Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bab84d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function run hypothesis testing algorithm\n",
    "def run_HT(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "        \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['HT'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        model = HT(config=HTConfig(adj_matrix_extended_pd))\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            if (abnormal_data_df[node] <100).any(): # Score instead of EoL\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "                results[node] = model.find_root_causes(abnormal_data_df, node, True).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for node in abnormal_nodes:\n",
    "            rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "            rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "            rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[node][0]['root_cause'])\n",
    "            root_cause_results.append(results[node][1]['root_cause'])\n",
    "            root_cause_results.append(results[node][2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_HT.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_HT.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_HT_overlap(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           overlap_p:float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "           \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results_overlap'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['HT'])\n",
    "    \n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        # Replace first N rows with overlap data\n",
    "        data_length = abnormal_data_df.shape[0]\n",
    "        overlap_n = int(np.rint(overlap_p*data_length))\n",
    "        overlap_df = normal_data_df.iloc[-overlap_n:]\n",
    "        abnormal_data_df = abnormal_data_df.iloc[-(data_length-overlap_n):]\n",
    "        \n",
    "        # Overlap of overlap_p timestamps\n",
    "        abnormal_data_df = pd.concat([overlap_df,abnormal_data_df], ignore_index=True)\n",
    "\n",
    "        model = HT(config=HTConfig(adj_matrix_extended_pd))\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            if (abnormal_data_df[node] <100).any(): # Score instead of EoL\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "                results[node] = model.find_root_causes(abnormal_data_df, node, True).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for node in abnormal_nodes:\n",
    "            rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "            rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "            rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[node][0]['root_cause'])\n",
    "            root_cause_results.append(results[node][1]['root_cause'])\n",
    "            root_cause_results.append(results[node][2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_HT.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_HT.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_HT_size(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           size_p:float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    # Resize normal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "    data_length = 526\n",
    "    size_n = int(np.rint(size_p*data_length))\n",
    "    # We take the first N timestamps\n",
    "    normal_data_df = normal_data_df.iloc[:size_n]\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['HT'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_data_df = abnormal_data_df.iloc[:size_n]\n",
    "\n",
    "        model = HT(config=HTConfig(adj_matrix_extended_pd))\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            if (abnormal_data_df[node] <100).any(): # Score instead of EoL\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "                results[node] = model.find_root_causes(abnormal_data_df, node, True).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for node in abnormal_nodes:\n",
    "            rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "            rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "            rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[node][0]['root_cause'])\n",
    "            root_cause_results.append(results[node][1]['root_cause'])\n",
    "            root_cause_results.append(results[node][2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_HT.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_HT.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_HT_normal_size(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           normal_size_p:float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    # Resize normal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "    data_length = 526\n",
    "    normal_size_n = int(np.rint(normal_size_p*data_length))\n",
    "    # We take the first N timestamps\n",
    "    normal_data_df = normal_data_df.iloc[:normal_size_n]\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['HT'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        model = HT(config=HTConfig(adj_matrix_extended_pd))\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            if (abnormal_data_df[node] <100).any(): # Score instead of EoL\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "                results[node] = model.find_root_causes(abnormal_data_df, node, True).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for node in abnormal_nodes:\n",
    "            rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "            rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "            rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[node][0]['root_cause'])\n",
    "            root_cause_results.append(results[node][1]['root_cause'])\n",
    "            root_cause_results.append(results[node][2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_HT.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_HT.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_HT_abnormal_size(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           abnormal_size_p:float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "     \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['HT'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        # Resize abnormal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "        data_length = 526\n",
    "        abnormal_size_n = int(np.rint(abnormal_size_p*data_length))\n",
    "        # We take the first N timestamps\n",
    "        abnormal_data_df = abnormal_data_df.iloc[:abnormal_size_n]\n",
    "\n",
    "        model = HT(config=HTConfig(adj_matrix_extended_pd))\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            if (abnormal_data_df[node] <100).any(): # Score instead of EoL\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "                results[node] = model.find_root_causes(abnormal_data_df, node, True).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for node in abnormal_nodes:\n",
    "            rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "            rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "            rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[node][0]['root_cause'])\n",
    "            root_cause_results.append(results[node][1]['root_cause'])\n",
    "            root_cause_results.append(results[node][2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_HT.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_HT.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_HT_edges_delete(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           edges_delete_n:int,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    edges_to_delete = random.sample(edges_list, edges_delete_n)\n",
    "    edges_list_new = edges_list.copy()\n",
    "\n",
    "    for edge in edges_to_delete:\n",
    "        edges_list_new.remove(edge)\n",
    "\n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list_new)\n",
    "    \n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['HT'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        model = HT(config=HTConfig(adj_matrix_extended_pd))\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            if (abnormal_data_df[node] <100).any(): # Score instead of EoL\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "                results[node] = model.find_root_causes(abnormal_data_df, node, True,).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for node in abnormal_nodes:\n",
    "            rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "            rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "            rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[node][0]['root_cause'])\n",
    "            root_cause_results.append(results[node][1]['root_cause'])\n",
    "            root_cause_results.append(results[node][2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_HT.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_HT.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f406611",
   "metadata": {},
   "source": [
    "#### ED - Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8492ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function run epsilon diagnosis algorithm\n",
    "def run_ED(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_ED.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    results_out = pd.DataFrame(index = ['ED'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        model = EpsilonDiagnosis(EpsilonDiagnosisConfig(alpha=0.05,root_cause_top_k=3,bootstrap_time=200))\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any():\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_data_df).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_ED.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_ED.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_ED_overlap(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           overlap_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results_overlap'\n",
    "            filename = f'{file_key}_ED.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['ED'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        # Replace first N rows with overlap data\n",
    "        data_length = abnormal_data_df.shape[0]\n",
    "        overlap_n = int(np.rint(overlap_p*data_length))\n",
    "        overlap_df = normal_data_df.iloc[-overlap_n:]\n",
    "        abnormal_data_df = abnormal_data_df.iloc[-(data_length-overlap_n):]\n",
    "        \n",
    "        # Overlap of overlap_p timestamps\n",
    "        abnormal_data_df = pd.concat([overlap_df,abnormal_data_df], ignore_index=True)\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        model = EpsilonDiagnosis(EpsilonDiagnosisConfig(alpha=0.05,root_cause_top_k=3,bootstrap_time=200))\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any():\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_data_df).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_ED.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_ED.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_ED_size(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           size_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "                    'gripper_2':'interGripper2',\n",
    "                    'max_Vel_1':'interVeloRob1',\n",
    "                    'max_Vel_2':'interVeloRob2',\n",
    "                    'camera_1':'interCamera1',\n",
    "                    'camera_2':'interCamera2',\n",
    "                    'camera_3':'interCamera3',\n",
    "                    'conveyor_1':'interConveyor1',\n",
    "                    'conveyor_2':'interConveyor2',\n",
    "                    'conveyor_3':'interConveyor3',\n",
    "                    'feeder_1':'interFeeder1',\n",
    "                    'feeder_2':'interFeeder2',\n",
    "                    'feeder_3':'interFeeder3',\n",
    "                    'size_1':'interSize1',\n",
    "                    'size_2':'interSize2',\n",
    "                    'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results_overlap'\n",
    "            filename = f'{file_key}_ED.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "\n",
    "    # Resize normal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "    data_length = 526\n",
    "    size_n = int(np.rint(size_p*data_length))\n",
    "    # We take the first N timestamps\n",
    "    normal_data_df = normal_data_df.iloc[:size_n]\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['ED'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_data_df = abnormal_data_df.iloc[:size_n]\n",
    "        \n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "        \n",
    "        model = EpsilonDiagnosis(EpsilonDiagnosisConfig(alpha=0.05,root_cause_top_k=3,bootstrap_time=200))\n",
    "        model.train(normal_data_df)\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any():\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_data_df).to_list()\n",
    "        \n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_ED.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_ED.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cb9a1",
   "metadata": {},
   "source": [
    "#### RW - Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0ede17da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function run random walk algorithm\n",
    "def run_RW(folder_path: str,\n",
    "           files: dict,\n",
    "           #train_file:pd.DataFrame,\n",
    "           startrow: int,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    results_out = pd.DataFrame(index = ['RW'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        model = RandomWalk(RandomWalkConfig(graph=adjacency_df,root_cause_top_k=3))\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any(): # from EoL to score\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_nodes,create_data_from_list([abnormal_data_df,normal_data_df])).to_list()\n",
    "\n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "        \n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RW.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_RW.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RW_overlap(folder_path: str,\n",
    "           files: dict,\n",
    "           #train_file:pd.DataFrame,\n",
    "           startrow: int,\n",
    "           overlap_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results_overlap'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    results_out = pd.DataFrame(index = ['RW'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        # Replace first N rows with overlap data\n",
    "        data_length = abnormal_data_df.shape[0]\n",
    "        overlap_n = int(np.rint(overlap_p*data_length))\n",
    "        overlap_df = normal_data_df.iloc[-overlap_n:]\n",
    "        abnormal_data_df = abnormal_data_df.iloc[-(data_length-overlap_n):]\n",
    "\n",
    "        # Overlap of overlap_p timestamps\n",
    "        abnormal_data_df = pd.concat([overlap_df,abnormal_data_df], ignore_index=True)\n",
    "\n",
    "        model = RandomWalk(RandomWalkConfig(graph=adjacency_df,root_cause_top_k=3))\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any(): # from EoL to score\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_nodes,create_data_from_list([abnormal_data_df,normal_data_df])).to_list()\n",
    "\n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "        \n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RW.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_RW.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RW_size(folder_path: str,\n",
    "           files: dict,\n",
    "           #train_file:pd.DataFrame,\n",
    "           startrow: int,\n",
    "           size_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    # Resize normal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "    data_length = 526\n",
    "    size_n = int(np.rint(size_p*data_length))\n",
    "    # We take the first N timestamps\n",
    "    normal_data_df = normal_data_df.iloc[:size_n]\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "    results_out = pd.DataFrame(index = ['RW'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_data_df = abnormal_data_df.iloc[:size_n]\n",
    "\n",
    "        model = RandomWalk(RandomWalkConfig(graph=adjacency_df,root_cause_top_k=3))\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any(): # from EoL to score\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_nodes,create_data_from_list([abnormal_data_df,normal_data_df])).to_list()\n",
    "\n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "        \n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RW.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_RW.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RW_normal_size(folder_path: str,\n",
    "           files: dict,\n",
    "           #train_file:pd.DataFrame,\n",
    "           startrow: int,\n",
    "           normal_size_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "\n",
    "    # Resize normal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "    data_length = 526\n",
    "    normal_size_n = int(np.rint(normal_size_p*data_length))\n",
    "    # We take the first N timestamps\n",
    "    normal_data_df = normal_data_df.iloc[:normal_size_n]\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    results_out = pd.DataFrame(index = ['RW'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        model = RandomWalk(RandomWalkConfig(graph=adjacency_df,root_cause_top_k=3))\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any(): # from EoL to score\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_nodes,create_data_from_list([abnormal_data_df,normal_data_df])).to_list()\n",
    "\n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "        \n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RW.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_RW.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RW_abnormal_size(folder_path: str,\n",
    "           files: dict,\n",
    "           #train_file:pd.DataFrame,\n",
    "           startrow: int,\n",
    "           abnormal_size_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    results_out = pd.DataFrame(index = ['RW'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        # Resize abnormal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "        data_length = 526\n",
    "        abnormal_size_n = int(np.rint(abnormal_size_p*data_length))\n",
    "        # We take the first N timestamps \n",
    "        abnormal_data_df = abnormal_data_df.iloc[:abnormal_size_n]\n",
    "        \n",
    "        model = RandomWalk(RandomWalkConfig(graph=adjacency_df,root_cause_top_k=3))\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any(): # from EoL to score\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_nodes,create_data_from_list([normal_data_df,abnormal_data_df])).to_list()\n",
    "\n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "        \n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RW.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_RW.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RW_edges_delete(folder_path: str,\n",
    "           files: dict,\n",
    "           #train_file:pd.DataFrame,\n",
    "           startrow: int,\n",
    "           edges_delete_n: int,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "\n",
    "    edges_to_delete = random.sample(edges_list, edges_delete_n)\n",
    "    edges_list_new = edges_list.copy()\n",
    "\n",
    "    for edge in edges_to_delete:\n",
    "        edges_list_new.remove(edge)\n",
    "\n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list_new)\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "    \n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path'])\n",
    "\n",
    "    results_out = pd.DataFrame(index = ['RW'])\n",
    "\n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        model = RandomWalk(RandomWalkConfig(graph=adjacency_df,root_cause_top_k=3))\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        for node in key_nodes:\n",
    "            # Basic Anomaly Detection\n",
    "            if (abnormal_data_df[node] <100).any(): # from EoL to score\n",
    "                abnormal_nodes.append(node)\n",
    "                new_colors[node] = 'yellow'\n",
    "\n",
    "        results = model.find_root_causes(abnormal_nodes,create_data_from_list([abnormal_data_df,normal_data_df])).to_list()\n",
    "\n",
    "        rank1_root_cause = []\n",
    "        rank2_root_cause = []\n",
    "        rank3_root_cause = []\n",
    "        \n",
    "\n",
    "        for i in range(0,int(np.trunc(len(results)/3))):\n",
    "            rank1_root_cause.append(results[i]['root_cause'])\n",
    "            rank2_root_cause.append(results[i+1]['root_cause'])\n",
    "            rank3_root_cause.append(results[i+2]['root_cause'])\n",
    "\n",
    "            root_cause_results.append(results[0]['root_cause'])\n",
    "            root_cause_results.append(results[1]['root_cause'])\n",
    "            root_cause_results.append(results[2]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RW.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "\n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "        \n",
    "        filename = f'{file_names[file_counter]}_RW.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361846d",
   "metadata": {},
   "source": [
    "#### RCD - Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e73cf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function run Root cause discovery algorithm\n",
    "def run_RCD(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "                \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path']) \n",
    "    results_out = pd.DataFrame(index = ['RCD'])\n",
    "    \n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            model = RCD(RCDConfig(bins=5,f_node=node,localized=True,k=3)) #alpha_step=0.05,start_alpha=0.001,alpha_limit=0.5\n",
    "            if (abnormal_data_df[node] < 100).any():\n",
    "                error = True\n",
    "                while error:\n",
    "                    abnormal_nodes.append(node)\n",
    "                    new_colors[node] = 'yellow'\n",
    "                    try:\n",
    "                        results[node] = model.find_root_causes(normal_data_df,abnormal_data_df).to_list()                        \n",
    "                        error = False\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            rank1_root_cause = []\n",
    "            rank2_root_cause = []\n",
    "            rank3_root_cause = []\n",
    "\n",
    "            if len(results[node]) == 3:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "                rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "                root_cause_results.append(results[node][2]['root_cause'])\n",
    "            elif len(results[node]) == 2:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "\n",
    "            elif len(results[node]) == 1:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RCD.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "    \n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_RCD.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RCD_overlap(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           overlap_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results_overlap'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "                \n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path']) \n",
    "    results_out = pd.DataFrame(index = ['RCD'])\n",
    "    \n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        # Replace first N rows with overlap data\n",
    "        data_length = abnormal_data_df.shape[0]\n",
    "        overlap_n = int(np.rint(overlap_p*data_length))\n",
    "        overlap_df = normal_data_df.iloc[-overlap_n:]\n",
    "        abnormal_data_df = abnormal_data_df.iloc[-(data_length-overlap_n):]\n",
    "        \n",
    "        # Overlap of overlap_p timestamps\n",
    "        abnormal_data_df = pd.concat([overlap_df,abnormal_data_df], ignore_index=True)\n",
    "        \n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            model = RCD(RCDConfig(bins=5,f_node=node,localized=True,k=3)) #alpha_step=0.05,start_alpha=0.001,alpha_limit=0.5\n",
    "            if (abnormal_data_df[node] < 100).any():\n",
    "                error = True\n",
    "                while error:\n",
    "                    abnormal_nodes.append(node)\n",
    "                    new_colors[node] = 'yellow'\n",
    "                    try:\n",
    "                        results[node] = model.find_root_causes(normal_data_df,abnormal_data_df).to_list()                        \n",
    "                        error = False\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            rank1_root_cause = []\n",
    "            rank2_root_cause = []\n",
    "            rank3_root_cause = []\n",
    "\n",
    "            if len(results[node]) == 3:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "                rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "                root_cause_results.append(results[node][2]['root_cause'])\n",
    "            elif len(results[node]) == 2:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "\n",
    "            elif len(results[node]) == 1:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RCD.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "    \n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_RCD.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RCD_size(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           size_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    # Resize normal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "    data_length = 526\n",
    "    size_n = int(np.rint(size_p*data_length))\n",
    "    # We take the first N timestamps\n",
    "    normal_data_df = normal_data_df.iloc[:size_n]\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path']) \n",
    "    results_out = pd.DataFrame(index = ['RCD'])\n",
    "    \n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_data_df = abnormal_data_df.iloc[:size_n]\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            model = RCD(RCDConfig(bins=5,f_node=node,localized=True,k=3)) #alpha_step=0.05,start_alpha=0.001,alpha_limit=0.5\n",
    "            if (abnormal_data_df[node] < 100).any():\n",
    "                error = True\n",
    "                while error:\n",
    "                    abnormal_nodes.append(node)\n",
    "                    new_colors[node] = 'yellow'\n",
    "                    try:\n",
    "                        results[node] = model.find_root_causes(normal_data_df,abnormal_data_df).to_list()                        \n",
    "                        error = False\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            rank1_root_cause = []\n",
    "            rank2_root_cause = []\n",
    "            rank3_root_cause = []\n",
    "\n",
    "            if len(results[node]) == 3:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "                rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "                root_cause_results.append(results[node][2]['root_cause'])\n",
    "            elif len(results[node]) == 2:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "\n",
    "            elif len(results[node]) == 1:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RCD.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "    \n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_RCD.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RCD_normal_size(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           normal_size_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "    \n",
    "    # Resize normal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "    data_length = 526\n",
    "    normal_size_n = int(np.rint(normal_size_p*data_length))\n",
    "    # We take the first N timestamps\n",
    "    normal_data_df = normal_data_df.iloc[:normal_size_n]\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path']) \n",
    "    results_out = pd.DataFrame(index = ['RCD'])\n",
    "    \n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            model = RCD(RCDConfig(bins=5,f_node=node,localized=True,k=3)) #alpha_step=0.05,start_alpha=0.001,alpha_limit=0.5\n",
    "            if (abnormal_data_df[node] < 100).any():\n",
    "                error = True\n",
    "                while error:\n",
    "                    abnormal_nodes.append(node)\n",
    "                    new_colors[node] = 'yellow'\n",
    "                    try:\n",
    "                        results[node] = model.find_root_causes(normal_data_df,abnormal_data_df).to_list()                        \n",
    "                        error = False\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            rank1_root_cause = []\n",
    "            rank2_root_cause = []\n",
    "            rank3_root_cause = []\n",
    "\n",
    "            if len(results[node]) == 3:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "                rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "                root_cause_results.append(results[node][2]['root_cause'])\n",
    "            elif len(results[node]) == 2:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "\n",
    "            elif len(results[node]) == 1:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RCD.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "    \n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_RCD.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n",
    "\n",
    "def run_RCD_abnormal_size(folder_path: str,\n",
    "           files: dict,\n",
    "           startrow: int,\n",
    "           abnormal_size_p: float,\n",
    "           nodes: list,\n",
    "           edges_list:list,\n",
    "           key_nodes: list,\n",
    "           colors: dict,\n",
    "           pos: dict):\n",
    "    \n",
    "    G_graph = nx.DiGraph()\n",
    "    G_graph.add_nodes_from(nodes)\n",
    "    G_graph.add_edges_from(edges_list)  # Make sure `edges_list` is defined somewhere\n",
    "\n",
    "    adj_matrix_extended_pd = nx.to_pandas_adjacency(G_graph, nodes)\n",
    "    adj_matrix_extended = nx.adjacency_matrix(G_graph,nodes).todense()\n",
    "    adjacency_df = pd.DataFrame(adj_matrix_extended, index=G_graph.nodes(), columns=G_graph.nodes())\n",
    "    \n",
    "    interventions = {'gripper_1':'interGripper1',\n",
    "         'gripper_2':'interGripper2',\n",
    "         'max_Vel_1':'interVeloRob1',\n",
    "         'max_Vel_2':'interVeloRob2',\n",
    "         'camera_1':'interCamera1',\n",
    "         'camera_2':'interCamera2',\n",
    "         'camera_3':'interCamera3',\n",
    "         'conveyor_1':'interConveyor1',\n",
    "         'conveyor_2':'interConveyor2',\n",
    "         'conveyor_3':'interConveyor3',\n",
    "         'feeder_1':'interFeeder1',\n",
    "         'feeder_2':'interFeeder2',\n",
    "         'feeder_3':'interFeeder3',\n",
    "         'size_1':'interSize1',\n",
    "         'size_2':'interSize2',\n",
    "         'size_3':'interSize3'}\n",
    "\n",
    "    abnormal_files = {}\n",
    "    file_names = []\n",
    "    counter = 0\n",
    "\n",
    "    for file_key, file_path in files.items():\n",
    "        temp_file_key = file_key.rsplit('-', 1)[0]\n",
    "        if temp_file_key == 'normal':\n",
    "            normal_data_df = pd.read_csv(file_path, skiprows=range(1, startrow))\n",
    "            normal_data_df = normal_data_df[nodes]\n",
    "            folder_name = 'Results'\n",
    "            filename = f'{file_key}_HT.png'\n",
    "            path = os.path.join(folder_path, folder_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name_save = os.path.join(path, filename)\n",
    "            #draw_save(G_graph, pos, colors, file_name_save, file_key)\n",
    "        \n",
    "        if temp_file_key in interventions:\n",
    "            counter += 1\n",
    "            abnormal_files[file_key] = file_path\n",
    "            file_names.append(file_key+'_'+str(counter))\n",
    "\n",
    "    abnormal_paths_df = pd.DataFrame.from_dict(abnormal_files, orient='index', columns=['file_path']) \n",
    "    results_out = pd.DataFrame(index = ['RCD'])\n",
    "    \n",
    "    for file_counter, abnormal_file_path in enumerate(abnormal_paths_df.values.flatten()):\n",
    "        abnormal_data_df = pd.read_csv(abnormal_file_path, skiprows=range(1, startrow))\n",
    "        abnormal_data_df = abnormal_data_df[nodes]\n",
    "\n",
    "        # Resize abnormal data relative to one cycle time - 26.3 equals 526 timestamps\n",
    "        data_length = 526\n",
    "        abnormal_size_n = int(np.rint(abnormal_size_p*data_length))\n",
    "        # We take the first N timestamps\n",
    "        abnormal_data_df = abnormal_data_df.iloc[:abnormal_size_n]\n",
    "\n",
    "        abnormal_nodes = []\n",
    "        new_colors = colors.copy()\n",
    "        root_cause_results = []\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "        for node in key_nodes:\n",
    "            model = RCD(RCDConfig(bins=5,f_node=node,localized=True,k=3)) #alpha_step=0.05,start_alpha=0.001,alpha_limit=0.5\n",
    "            if (abnormal_data_df[node] < 100).any():\n",
    "                error = True\n",
    "                while error:\n",
    "                    abnormal_nodes.append(node)\n",
    "                    new_colors[node] = 'yellow'\n",
    "                    try:\n",
    "                        results[node] = model.find_root_causes(normal_data_df,abnormal_data_df).to_list()                        \n",
    "                        error = False\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            rank1_root_cause = []\n",
    "            rank2_root_cause = []\n",
    "            rank3_root_cause = []\n",
    "\n",
    "            if len(results[node]) == 3:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "                rank3_root_cause.append(results[node][2]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "                root_cause_results.append(results[node][2]['root_cause'])\n",
    "            elif len(results[node]) == 2:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                rank2_root_cause.append(results[node][1]['root_cause'])\n",
    "\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][1]['root_cause'])\n",
    "\n",
    "            elif len(results[node]) == 1:\n",
    "                rank1_root_cause.append(results[node][0]['root_cause'])\n",
    "                root_cause_results.append(results[node][0]['root_cause'])\n",
    "\n",
    "        results_out[file_names[file_counter]] = [root_cause_results]\n",
    "        results_file_name = f'results_{file_names[file_counter]}_RCD.csv'\n",
    "        results_out.to_csv(os.path.join(path, results_file_name), index=False)\n",
    "    \n",
    "        for node in rank1_root_cause:\n",
    "            new_colors[node] = 'red'\n",
    "\n",
    "        for node in rank2_root_cause:\n",
    "            new_colors[node] = 'crimson'\n",
    "\n",
    "        for node in rank3_root_cause:\n",
    "            new_colors[node] = 'lightcoral'\n",
    "\n",
    "        filename = f'{file_names[file_counter]}_RCD.png'\n",
    "\n",
    "        file_name_save = os.path.join(path, filename)\n",
    "\n",
    "        #draw_save(G_graph, pos, new_colors, file_name_save, file_names[file_counter])\n",
    "\n",
    "    return results_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2adbc",
   "metadata": {},
   "source": [
    "## Create the Causal Graph (non-lagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb8781",
   "metadata": {},
   "source": [
    "### Graph Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "360ddf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "nodes = [\n",
    "    'cam_1_X', 'cam_2_X', 'cam_3_X',\n",
    "    'cam_1_Y', 'cam_2_Y', 'cam_3_Y',\n",
    "    'EoL_1_X', 'EoL_2_X', 'EoL_3_X', 'EoL_4_X', 'EoL_5_X', 'EoL_6_X',\n",
    "    'EoL_1_Y', 'EoL_2_Y', 'EoL_3_Y', 'EoL_4_Y', 'EoL_5_Y', 'EoL_6_Y',\n",
    "    'rob_1_1', 'rob_1_2', 'rob_1_3', 'rob_1_4', 'rob_1_maxVel',\n",
    "    'rob_2_1', 'rob_2_2', 'rob_2_3', 'rob_2_4', 'rob_2_maxVel',\n",
    "    'rob_1_supply', 'rob_2_supply',\n",
    "    'rob_1_vacuum', 'rob_2_vacuum',\n",
    "    'con_1','con_2','con_3',\n",
    "    'score'\n",
    "]\n",
    "\n",
    "\n",
    "edges = [\n",
    "    ('cam_1_X', 'rob_2_1'), ('cam_1_Y', 'rob_2_1'),\n",
    "    ('cam_1_X', 'rob_2_2'), ('cam_1_Y', 'rob_2_2'),\n",
    "    ('cam_1_X', 'rob_2_3'), ('cam_1_Y', 'rob_2_3'),\n",
    "    ('cam_1_X', 'rob_2_4'), ('cam_1_Y', 'rob_2_4'),\n",
    "    \n",
    "    ('cam_2_X', 'rob_1_1'), ('cam_2_Y', 'rob_1_1'),\n",
    "    ('cam_2_X', 'rob_1_2'), ('cam_2_Y', 'rob_1_2'),\n",
    "    ('cam_2_X', 'rob_1_3'), ('cam_2_Y', 'rob_1_3'),\n",
    "    ('cam_2_X', 'rob_1_4'), ('cam_2_Y', 'rob_1_4'),\n",
    "    \n",
    "    ('cam_3_X', 'rob_1_1'), ('cam_3_Y', 'rob_1_1'),\n",
    "    ('cam_3_X', 'rob_1_2'), ('cam_3_Y', 'rob_1_2'),\n",
    "    ('cam_3_X', 'rob_1_3'), ('cam_3_Y', 'rob_1_3'),\n",
    "    ('cam_3_X', 'rob_1_4'), ('cam_3_Y', 'rob_1_4'),\n",
    "    \n",
    "    ('rob_1_maxVel', 'rob_1_1'), ('rob_1_maxVel', 'rob_1_2'),\n",
    "    ('rob_1_maxVel', 'rob_1_3'), ('rob_1_maxVel', 'rob_1_4'),\n",
    "    \n",
    "    ('rob_2_maxVel', 'rob_2_1'), ('rob_2_maxVel', 'rob_2_2'),\n",
    "    ('rob_2_maxVel', 'rob_2_3'), ('rob_2_maxVel', 'rob_2_4'),\n",
    "    \n",
    "    ('con_2', 'rob_1_1'), ('con_2', 'rob_1_2'), ('con_2', 'rob_1_3'), ('con_2', 'rob_1_4'),\n",
    "    ('con_3', 'rob_1_1'), ('con_3', 'rob_1_2'), ('con_3', 'rob_1_3'), ('con_3', 'rob_1_4'),\n",
    "\n",
    "    ('con_2', 'rob_2_1'), ('con_2', 'rob_2_2'), ('con_2', 'rob_2_3'), ('con_2', 'rob_2_4'),\n",
    "    ('con_1', 'rob_2_1'), ('con_1', 'rob_2_2'), ('con_1', 'rob_2_3'), ('con_1', 'rob_2_4'),\n",
    "\n",
    "    ('con_2', 'EoL_1_X'), ('con_2', 'EoL_1_Y'),\n",
    "    \n",
    "    ('rob_1_1', 'rob_2_1'), ('rob_1_1', 'rob_2_2'), ('rob_1_1', 'rob_2_3'), ('rob_1_1', 'rob_2_4'),\n",
    "    ('rob_1_2', 'rob_2_1'), ('rob_1_2', 'rob_2_2'), ('rob_1_2', 'rob_2_3'), ('rob_1_2', 'rob_2_4'),\n",
    "    ('rob_1_3', 'rob_2_1'), ('rob_1_3', 'rob_2_2'), ('rob_1_3', 'rob_2_3'), ('rob_1_3', 'rob_2_4'),\n",
    "    ('rob_1_4', 'rob_2_1'), ('rob_1_4', 'rob_2_2'), ('rob_1_4', 'rob_2_3'), ('rob_1_4', 'rob_2_4'),\n",
    "\n",
    "    ('rob_1_supply', 'rob_1_vacuum'), \n",
    "    ('rob_2_supply', 'rob_2_vacuum'),\n",
    "\n",
    "    \n",
    "    ('rob_1_vacuum', 'rob_2_1'), ('rob_1_vacuum', 'rob_2_2'),\n",
    "    ('rob_1_vacuum', 'rob_2_3'), ('rob_1_vacuum', 'rob_2_4'),\n",
    "\n",
    "    ('rob_1_1', 'EoL_2_X'), ('rob_1_2', 'EoL_2_X'),\n",
    "    ('rob_1_3', 'EoL_2_X'), ('rob_1_4', 'EoL_2_X'),\n",
    "    ('rob_1_1', 'EoL_2_Y'), ('rob_1_2', 'EoL_2_Y'),\n",
    "    ('rob_1_3', 'EoL_2_Y'), ('rob_1_4', 'EoL_2_Y'),\n",
    "    \n",
    "    ('rob_2_1', 'EoL_3_X'), ('rob_2_2', 'EoL_3_X'),\n",
    "    ('rob_2_3', 'EoL_3_X'), ('rob_2_4', 'EoL_3_X'),\n",
    "    ('rob_2_1', 'EoL_3_Y'), ('rob_2_2', 'EoL_3_Y'),\n",
    "    ('rob_2_3', 'EoL_3_Y'), ('rob_2_4', 'EoL_3_Y'),\n",
    "    \n",
    "    ('rob_2_1', 'EoL_4_X'), ('rob_2_2', 'EoL_4_X'),\n",
    "    ('rob_2_3', 'EoL_4_X'), ('rob_2_4', 'EoL_4_X'),\n",
    "    ('rob_2_1', 'EoL_4_Y'), ('rob_2_2', 'EoL_4_Y'),\n",
    "    ('rob_2_3', 'EoL_4_Y'), ('rob_2_4', 'EoL_4_Y'),\n",
    "    \n",
    "    ('rob_2_1', 'EoL_5_X'), ('rob_2_2', 'EoL_5_X'),\n",
    "    ('rob_2_3', 'EoL_5_X'), ('rob_2_4', 'EoL_5_X'),\n",
    "    ('rob_2_1', 'EoL_5_Y'), ('rob_2_2', 'EoL_5_Y'),\n",
    "    ('rob_2_3', 'EoL_5_Y'), ('rob_2_4', 'EoL_5_Y'),\n",
    "\n",
    "    ('rob_2_1', 'EoL_6_X'), ('rob_2_2', 'EoL_6_X'),\n",
    "    ('rob_2_3', 'EoL_6_X'), ('rob_2_4', 'EoL_6_X'),\n",
    "    ('rob_2_1', 'EoL_6_Y'), ('rob_2_2', 'EoL_6_Y'),\n",
    "    ('rob_2_3', 'EoL_6_Y'), ('rob_2_4', 'EoL_6_Y'),\n",
    "\n",
    "    ('rob_1_vacuum', 'EoL_2_X'), ('rob_1_vacuum', 'EoL_2_Y'),\n",
    "    \n",
    "    ('rob_2_vacuum', 'EoL_3_X'), ('rob_2_vacuum', 'EoL_3_Y'),\n",
    "    ('rob_2_vacuum', 'EoL_4_X'), ('rob_2_vacuum', 'EoL_4_Y'),\n",
    "    ('rob_2_vacuum', 'EoL_5_X'), ('rob_2_vacuum', 'EoL_5_Y'),\n",
    "    ('rob_2_vacuum', 'EoL_6_X'), ('rob_2_vacuum', 'EoL_6_Y'),\n",
    "\n",
    "    ('EoL_1_X','score'), ('EoL_2_X','score'), ('EoL_3_X','score'), ('EoL_4_X','score'), ('EoL_5_X','score'), ('EoL_6_X','score'),\n",
    "    ('EoL_1_Y','score'), ('EoL_2_Y','score'), ('EoL_3_Y','score'), ('EoL_4_Y','score'), ('EoL_5_Y','score'), ('EoL_6_Y','score')\n",
    "]\n",
    "\n",
    "print(len(edges))\n",
    "\n",
    "check_nodes = ['score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6da23",
   "metadata": {},
   "source": [
    "### Additional Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ccef0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {\n",
    "    'cam_1_X':(8,4), 'cam_2_X':(-9,6), 'cam_3_X':(-5,6),\n",
    "    'cam_1_Y':(8,2), 'cam_2_Y':(-7,6), 'cam_3_Y':(-3,6),\n",
    "    'EoL_1_X':(10,-8), 'EoL_2_X':(-10,-8), 'EoL_3_X':(-6,-8), 'EoL_4_X':(-2,-8), 'EoL_5_X':(2,-8), 'EoL_6_X':(6,-8),\n",
    "    'EoL_1_Y':(12,-8), 'EoL_2_Y':(-8,-8), 'EoL_3_Y':(-4,-8), 'EoL_4_Y':(0,-8), 'EoL_5_Y':(4,-8), 'EoL_6_Y':(8,-8),\n",
    "    'score':(0,-10),\n",
    "    'rob_2_1':(-6,-4), 'rob_2_2':(-4,-4), 'rob_2_3':(-2,-4), 'rob_2_4':(-0,-4), 'rob_2_maxVel':(2,-4),\n",
    "    'rob_1_1':(-9,1), 'rob_1_2':(-7,1), 'rob_1_3':(-5,1), 'rob_1_4':(-3,1), 'rob_1_maxVel':(-1,1),\n",
    "    'rob_1_vacuum':(2,1), 'rob_2_vacuum':(5,-4),'rob_1_supply':(5,1), 'rob_2_supply':(8,-4),\n",
    "    'con_1':(8,-1),'con_2':(11,6),'con_3':(3,6)\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'cam_1_X':'skyblue', 'cam_2_X':'skyblue', 'cam_3_X':'skyblue',\n",
    "    'cam_1_Y':'skyblue', 'cam_2_Y':'skyblue', 'cam_3_Y':'skyblue',\n",
    "    'EoL_1_X':'lightgreen', 'EoL_2_X':'lightgreen', 'EoL_3_X':'lightgreen', 'EoL_4_X':'lightgreen', 'EoL_5_X':'lightgreen', 'EoL_6_X':'lightgreen',\n",
    "    'EoL_1_Y':'lightgreen', 'EoL_2_Y':'lightgreen', 'EoL_3_Y':'lightgreen', 'EoL_4_Y':'lightgreen', 'EoL_5_Y':'lightgreen', 'EoL_6_Y':'lightgreen',\n",
    "    'score':'lightsalmon',\n",
    "    'rob_1_1':'tan', 'rob_1_2':'tan', 'rob_1_3':'tan', 'rob_1_4':'tan', 'rob_1_maxVel':'tan',\n",
    "    'rob_2_1':'tan', 'rob_2_2':'tan', 'rob_2_3':'tan', 'rob_2_4':'tan', 'rob_2_maxVel':'tan',\n",
    "    'rob_1_vacuum':'tan', 'rob_2_vacuum':'tan','rob_1_supply':'tan', 'rob_2_supply':'tan',\n",
    "    'con_1':'lightgrey','con_2':'lightgrey','con_3':'lightgrey'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c55d6",
   "metadata": {},
   "source": [
    "## Show datasets in folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2778c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "directory_path = 'G:\\\\My Drive\\\\Master Thesis\\\\Simulation\\\\Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "07d1e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files,folder_path = get_from_folders(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa24f2",
   "metadata": {},
   "source": [
    "## Run RCA trough folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A warm-up phase is considered. Each product takes roughly 26.30 seconds from the assembly entry point to the assembly end of line. There is some distance from the drop point of the items to the start and some interventions need time to become stable, thus we need to skip in total roughly 839 rows. Each 8 seconds a new product exits the assembly line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c8120",
   "metadata": {},
   "source": [
    "### Algorithm #1 - Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f957d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_HT = run_HT_size(folder_path,files,startrow=839,size_p=2.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663bf75",
   "metadata": {},
   "source": [
    "### Algorithm #2 - Epsilon Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3dbec1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ED = run_ED_size(folder_path,files,startrow=839,size_p=2.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a7998",
   "metadata": {},
   "source": [
    "### Algorithm #3 - Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f197b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RW = run_RW_size(folder_path,files,startrow=839,size_p=2.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294504a",
   "metadata": {},
   "source": [
    "### Algorithm #4: RCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "401cf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RCD = run_RCD_size(folder_path,files,startrow=839,size_p=2.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97be24",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d5435",
   "metadata": {},
   "source": [
    "### Summary Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "edd47ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_1-1.2_1</th>\n",
       "      <th>feeder_3-1.3_2</th>\n",
       "      <th>gripper_1-1.4_3</th>\n",
       "      <th>max_Vel_2-1.5_4</th>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <th>size_1-2.2_9</th>\n",
       "      <th>feeder_3-2.3_10</th>\n",
       "      <th>...</th>\n",
       "      <th>gripper_1-7.8_55</th>\n",
       "      <th>max_Vel_2-7.9_56</th>\n",
       "      <th>size_1-8.2_57</th>\n",
       "      <th>feeder_3-8.3_58</th>\n",
       "      <th>gripper_1-8.4_59</th>\n",
       "      <th>max_Vel_2-8.5_60</th>\n",
       "      <th>size_1-8.6_61</th>\n",
       "      <th>feeder_3-8.7_62</th>\n",
       "      <th>gripper_1-8.8_63</th>\n",
       "      <th>max_Vel_2-8.9_64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HT</th>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, EoL_4_Y]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_2]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, EoL_4_Y]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_2]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, EoL_4_Y]</td>\n",
       "      <td>...</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_2]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, EoL_4_Y]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_1]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_1, rob_1_3]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, EoL_4_Y]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ED</th>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "      <td>[cam_2_Y, EoL_1_Y, con_1]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_2]</td>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "      <td>[EoL_1_Y, con_2, con_3]</td>\n",
       "      <td>[cam_3_Y, rob_1_4, rob_1_3]</td>\n",
       "      <td>[EoL_1_X, rob_1_vacuum, rob_1_4]</td>\n",
       "      <td>[cam_3_Y, rob_1_vacuum, rob_1_4]</td>\n",
       "      <td>[EoL_1_Y, EoL_1_X, con_2]</td>\n",
       "      <td>...</td>\n",
       "      <td>[cam_3_Y, rob_1_4, rob_1_3]</td>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "      <td>[cam_3_Y, rob_1_vacuum, rob_1_4]</td>\n",
       "      <td>[EoL_1_Y, con_3, cam_2_Y]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_2]</td>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "      <td>[EoL_1_X, rob_1_vacuum, rob_1_4]</td>\n",
       "      <td>[EoL_1_Y, con_2, cam_2_Y]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_2]</td>\n",
       "      <td>[cam_3_Y, rob_1_vacuum, rob_1_4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW</th>\n",
       "      <td>[cam_1_X, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_2_supply, rob_1_supply, cam_1_Y]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_2_supply, cam_1_X, cam_1_Y]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_2_supply, cam_1_X, cam_1_Y]</td>\n",
       "      <td>[rob_1_supply, cam_1_Y, rob_2_supply]</td>\n",
       "      <td>...</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, rob_2_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_2_supply, cam_1_X, cam_1_Y]</td>\n",
       "      <td>[rob_1_supply, rob_2_maxVel, rob_2_supply]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_2_supply, cam_1_X, cam_1_Y]</td>\n",
       "      <td>[rob_2_supply, rob_1_supply, cam_1_Y]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_X]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RCD</th>\n",
       "      <td>[EoL_3_Y, cam_1_X, EoL_3_X]</td>\n",
       "      <td>[cam_3_Y, EoL_4_Y, EoL_4_X]</td>\n",
       "      <td>[EoL_6_X, EoL_5_Y]</td>\n",
       "      <td>[rob_2_maxVel, EoL_6_Y]</td>\n",
       "      <td>[cam_1_Y, EoL_6_Y]</td>\n",
       "      <td>[EoL_3_X, cam_3_X]</td>\n",
       "      <td>[EoL_3_X, EoL_2_X]</td>\n",
       "      <td>[EoL_6_Y, EoL_6_X]</td>\n",
       "      <td>[EoL_5_X, EoL_4_Y]</td>\n",
       "      <td>[EoL_2_Y, EoL_6_Y]</td>\n",
       "      <td>...</td>\n",
       "      <td>[EoL_4_X, EoL_5_Y]</td>\n",
       "      <td>[EoL_6_X, rob_2_maxVel]</td>\n",
       "      <td>[cam_1_X, EoL_3_X, EoL_3_Y]</td>\n",
       "      <td>[EoL_4_X, EoL_2_Y]</td>\n",
       "      <td>[EoL_3_X, EoL_5_Y]</td>\n",
       "      <td>[EoL_6_X]</td>\n",
       "      <td>[EoL_3_X, EoL_6_X, EoL_4_X]</td>\n",
       "      <td>[EoL_4_X, cam_3_X]</td>\n",
       "      <td>[EoL_6_Y, EoL_2_X, EoL_5_X]</td>\n",
       "      <td>[rob_2_maxVel, EoL_6_X]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows  64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         size_1-1.2_1                         feeder_3-1.3_2  \\\n",
       "HT   [rob_1_maxVel, rob_1_3, rob_1_1]  [rob_2_maxVel, rob_1_maxVel, EoL_4_Y]   \n",
       "ED   [rob_1_vacuum, rob_1_4, rob_1_3]              [cam_2_Y, EoL_1_Y, con_1]   \n",
       "RW   [cam_1_X, rob_2_supply, cam_1_Y]  [rob_2_supply, rob_1_supply, cam_1_Y]   \n",
       "RCD       [EoL_3_Y, cam_1_X, EoL_3_X]            [cam_3_Y, EoL_4_Y, EoL_4_X]   \n",
       "\n",
       "                                gripper_1-1.4_3  \\\n",
       "HT   [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "ED                  [rob_1_4, rob_1_3, rob_1_2]   \n",
       "RW        [rob_1_supply, rob_2_supply, cam_1_Y]   \n",
       "RCD                          [EoL_6_X, EoL_5_Y]   \n",
       "\n",
       "                                max_Vel_2-1.5_4  \\\n",
       "HT   [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "ED             [rob_1_vacuum, rob_1_4, rob_1_3]   \n",
       "RW        [rob_2_maxVel, rob_2_supply, cam_1_Y]   \n",
       "RCD                     [rob_2_maxVel, EoL_6_Y]   \n",
       "\n",
       "                         size_1-1.6_5                         feeder_3-1.7_6  \\\n",
       "HT   [rob_1_maxVel, rob_1_3, rob_1_2]  [rob_2_maxVel, rob_1_maxVel, EoL_4_Y]   \n",
       "ED   [rob_1_vacuum, rob_1_4, rob_1_3]                [EoL_1_Y, con_2, con_3]   \n",
       "RW   [rob_2_supply, cam_1_X, cam_1_Y]  [rob_1_supply, rob_2_supply, cam_1_X]   \n",
       "RCD                [cam_1_Y, EoL_6_Y]                     [EoL_3_X, cam_3_X]   \n",
       "\n",
       "                                gripper_1-1.8_7  \\\n",
       "HT   [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "ED                  [cam_3_Y, rob_1_4, rob_1_3]   \n",
       "RW        [rob_1_supply, rob_2_supply, cam_1_X]   \n",
       "RCD                          [EoL_3_X, EoL_2_X]   \n",
       "\n",
       "                                max_Vel_2-1.9_8  \\\n",
       "HT   [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "ED             [EoL_1_X, rob_1_vacuum, rob_1_4]   \n",
       "RW        [rob_2_maxVel, rob_2_supply, cam_1_X]   \n",
       "RCD                          [EoL_6_Y, EoL_6_X]   \n",
       "\n",
       "                         size_1-2.2_9                        feeder_3-2.3_10  \\\n",
       "HT   [rob_1_maxVel, rob_1_3, rob_1_2]  [rob_2_maxVel, rob_1_maxVel, EoL_4_Y]   \n",
       "ED   [cam_3_Y, rob_1_vacuum, rob_1_4]              [EoL_1_Y, EoL_1_X, con_2]   \n",
       "RW   [rob_2_supply, cam_1_X, cam_1_Y]  [rob_1_supply, cam_1_Y, rob_2_supply]   \n",
       "RCD                [EoL_5_X, EoL_4_Y]                     [EoL_2_Y, EoL_6_Y]   \n",
       "\n",
       "     ...                            gripper_1-7.8_55  \\\n",
       "HT   ...  [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "ED   ...                 [cam_3_Y, rob_1_4, rob_1_3]   \n",
       "RW   ...  [rob_1_supply, rob_2_supply, rob_2_maxVel]   \n",
       "RCD  ...                          [EoL_4_X, EoL_5_Y]   \n",
       "\n",
       "                               max_Vel_2-7.9_56  \\\n",
       "HT   [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "ED             [rob_1_vacuum, rob_1_4, rob_1_3]   \n",
       "RW        [rob_2_maxVel, rob_2_supply, cam_1_X]   \n",
       "RCD                     [EoL_6_X, rob_2_maxVel]   \n",
       "\n",
       "                        size_1-8.2_57  \\\n",
       "HT   [rob_1_maxVel, rob_1_3, rob_1_2]   \n",
       "ED   [cam_3_Y, rob_1_vacuum, rob_1_4]   \n",
       "RW   [rob_2_supply, cam_1_X, cam_1_Y]   \n",
       "RCD       [cam_1_X, EoL_3_X, EoL_3_Y]   \n",
       "\n",
       "                                feeder_3-8.3_58  \\\n",
       "HT        [rob_2_maxVel, rob_1_maxVel, EoL_4_Y]   \n",
       "ED                    [EoL_1_Y, con_3, cam_2_Y]   \n",
       "RW   [rob_1_supply, rob_2_maxVel, rob_2_supply]   \n",
       "RCD                          [EoL_4_X, EoL_2_Y]   \n",
       "\n",
       "                          gripper_1-8.4_59  \\\n",
       "HT   [rob_1_supply, rob_1_vacuum, rob_1_1]   \n",
       "ED             [rob_1_4, rob_1_3, rob_1_2]   \n",
       "RW   [rob_1_supply, rob_2_supply, cam_1_X]   \n",
       "RCD                     [EoL_3_X, EoL_5_Y]   \n",
       "\n",
       "                               max_Vel_2-8.5_60  \\\n",
       "HT   [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "ED             [rob_1_vacuum, rob_1_4, rob_1_3]   \n",
       "RW        [rob_2_maxVel, rob_2_supply, cam_1_Y]   \n",
       "RCD                                   [EoL_6_X]   \n",
       "\n",
       "                        size_1-8.6_61                        feeder_3-8.7_62  \\\n",
       "HT   [rob_1_maxVel, rob_1_1, rob_1_3]  [rob_2_maxVel, rob_1_maxVel, EoL_4_Y]   \n",
       "ED   [EoL_1_X, rob_1_vacuum, rob_1_4]              [EoL_1_Y, con_2, cam_2_Y]   \n",
       "RW   [rob_2_supply, cam_1_X, cam_1_Y]  [rob_2_supply, rob_1_supply, cam_1_Y]   \n",
       "RCD       [EoL_3_X, EoL_6_X, EoL_4_X]                     [EoL_4_X, cam_3_X]   \n",
       "\n",
       "                               gripper_1-8.8_63  \\\n",
       "HT   [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "ED                  [rob_1_4, rob_1_3, rob_1_2]   \n",
       "RW        [rob_1_supply, rob_2_supply, cam_1_Y]   \n",
       "RCD                 [EoL_6_Y, EoL_2_X, EoL_5_X]   \n",
       "\n",
       "                               max_Vel_2-8.9_64  \n",
       "HT   [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]  \n",
       "ED             [cam_3_Y, rob_1_vacuum, rob_1_4]  \n",
       "RW        [rob_2_maxVel, rob_2_supply, cam_1_X]  \n",
       "RCD                     [rob_2_maxVel, EoL_6_X]  \n",
       "\n",
       "[4 rows x 64 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_all = pd.DataFrame()\n",
    "result_all = pd.concat([result_HT,result_ED,result_RW,result_RCD])\n",
    "\n",
    "result_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421b281",
   "metadata": {},
   "source": [
    "###  Intervention - Root Cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7b8bdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_sets = pd.DataFrame()\n",
    "abnormal_sets['size_1'] = ['cam_1_X', 'cam_1_Y']\n",
    "abnormal_sets['feeder_3'] = ['cam_3_X', 'cam_3_Y']\n",
    "abnormal_sets['gripper_1'] = ['rob_1_supply',None]\n",
    "abnormal_sets['max_Vel_2'] = ['rob_2_maxVel',None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80492867",
   "metadata": {},
   "source": [
    "### TOP 3  - Root Cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ac209090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HT</th>\n",
       "      <th>ED</th>\n",
       "      <th>RW</th>\n",
       "      <th>RCD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-8.6_61</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-8.7_62</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-8.8_63</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-8.9_64</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        HT   ED       RW  RCD\n",
       "size_1-8.6_61     0.000000  0.0  1.00000  0.0\n",
       "feeder_3-8.7_62   0.000000  0.0  0.00000  1.0\n",
       "gripper_1-8.8_63  1.000000  0.0  1.00000  0.0\n",
       "max_Vel_2-8.9_64  1.000000  0.0  1.00000  1.0\n",
       "Total             0.578125  0.0  0.78125  0.5"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_3top = results_top_3(result_all,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41995172",
   "metadata": {},
   "source": [
    "### TOP 1  - Root Cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "56f535fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HT</th>\n",
       "      <th>ED</th>\n",
       "      <th>RW</th>\n",
       "      <th>RCD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-8.6_61</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-8.7_62</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-8.8_63</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-8.9_64</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.265625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   HT   ED        RW       RCD\n",
       "size_1-8.6_61     0.0  0.0  0.000000  0.000000\n",
       "feeder_3-8.7_62   0.0  0.0  0.000000  0.000000\n",
       "gripper_1-8.8_63  1.0  0.0  1.000000  0.000000\n",
       "max_Vel_2-8.9_64  1.0  0.0  1.000000  1.000000\n",
       "Total             0.5  0.0  0.578125  0.265625"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_1top = results_top_1(result_all,abnormal_sets)\n",
    "result_data_1top.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fece38",
   "metadata": {},
   "source": [
    "## Variation 1 - Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5aafce",
   "metadata": {},
   "source": [
    "### HT -Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ddc2b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_HT_overlap_1 = run_HT_overlap(folder_path,files,startrow=839,overlap_p=0.10,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_overlap_2 = run_HT_overlap(folder_path,files,startrow=839,overlap_p=0.20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_overlap_3 = run_HT_overlap(folder_path,files,startrow=839,overlap_p=0.50,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_overlap_4 = run_HT_overlap(folder_path,files,startrow=839,overlap_p=0.75,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_overlap_5 = run_HT_overlap(folder_path,files,startrow=839,overlap_p=0.95,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2e2c405e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_1-1.2_1</th>\n",
       "      <th>feeder_3-1.3_2</th>\n",
       "      <th>gripper_1-1.4_3</th>\n",
       "      <th>max_Vel_2-1.5_4</th>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <th>size_1-2.2_9</th>\n",
       "      <th>feeder_3-2.3_10</th>\n",
       "      <th>...</th>\n",
       "      <th>gripper_1-7.8_55</th>\n",
       "      <th>max_Vel_2-7.9_56</th>\n",
       "      <th>size_1-8.2_57</th>\n",
       "      <th>feeder_3-8.3_58</th>\n",
       "      <th>gripper_1-8.4_59</th>\n",
       "      <th>max_Vel_2-8.5_60</th>\n",
       "      <th>size_1-8.6_61</th>\n",
       "      <th>feeder_3-8.7_62</th>\n",
       "      <th>gripper_1-8.8_63</th>\n",
       "      <th>max_Vel_2-8.9_64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overlap Percent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_2_3]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_2_3]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_2_3]</td>\n",
       "      <td>...</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_2_3]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_2_3]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>...</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>...</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>...</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>...</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               size_1-1.2_1  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                             feeder_3-1.3_2  \\\n",
       "Overlap Percent                                               \n",
       "10%                   [rob_2_maxVel, rob_1_maxVel, rob_2_3]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                            gripper_1-1.4_3  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "20%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "50%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "75%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "95%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "\n",
       "                                            max_Vel_2-1.5_4  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                               size_1-1.6_5  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                             feeder_3-1.7_6  \\\n",
       "Overlap Percent                                               \n",
       "10%                   [rob_2_maxVel, rob_1_maxVel, rob_2_3]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                            gripper_1-1.8_7  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "20%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "50%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "75%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "95%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "\n",
       "                                            max_Vel_2-1.9_8  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                               size_1-2.2_9  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                            feeder_3-2.3_10  ...  \\\n",
       "Overlap Percent                                              ...   \n",
       "10%                   [rob_2_maxVel, rob_1_maxVel, rob_2_3]  ...   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]  ...   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]  ...   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]  ...   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]  ...   \n",
       "\n",
       "                                           gripper_1-7.8_55  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "20%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "50%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "75%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "95%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "\n",
       "                                           max_Vel_2-7.9_56  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                              size_1-8.2_57  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                            feeder_3-8.3_58  \\\n",
       "Overlap Percent                                               \n",
       "10%                   [rob_2_maxVel, rob_1_maxVel, rob_2_3]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                           gripper_1-8.4_59  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "20%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "50%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "75%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "95%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "\n",
       "                                           max_Vel_2-8.5_60  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                              size_1-8.6_61  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                            feeder_3-8.7_62  \\\n",
       "Overlap Percent                                               \n",
       "10%                   [rob_2_maxVel, rob_1_maxVel, rob_2_3]   \n",
       "20%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "50%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "75%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "95%              [rob_1_maxVel, rob_2_maxVel, rob_1_vacuum]   \n",
       "\n",
       "                                           gripper_1-8.8_63  \\\n",
       "Overlap Percent                                               \n",
       "10%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "20%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "50%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "75%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "95%              [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "\n",
       "                                           max_Vel_2-8.9_64  \n",
       "Overlap Percent                                              \n",
       "10%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]  \n",
       "20%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]  \n",
       "50%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]  \n",
       "75%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]  \n",
       "95%              [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_overlap_HT = pd.DataFrame()\n",
    "result_overlap_HT = pd.concat([result_HT_overlap_1,result_HT_overlap_2,result_HT_overlap_3,result_HT_overlap_4,result_HT_overlap_5], ignore_index=True)\n",
    "result_overlap_HT['Overlap Percent'] = pd.DataFrame({'HT':['10%','20%','50%','75%','95%']})\n",
    "result_overlap_HT.set_index('Overlap Percent', inplace=True)\n",
    "result_overlap_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "90775c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10%</th>\n",
       "      <th>20%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>95%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-8.6_61</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-8.7_62</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-8.8_63</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-8.9_64</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  10%  20%  50%  75%  95%\n",
       "size_1-8.6_61     0.0  0.0  0.0  0.0  0.0\n",
       "feeder_3-8.7_62   0.0  0.0  0.0  0.0  0.0\n",
       "gripper_1-8.8_63  1.0  1.0  1.0  1.0  1.0\n",
       "max_Vel_2-8.9_64  1.0  1.0  1.0  1.0  1.0\n",
       "Total             0.5  0.5  0.5  0.5  0.5"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_3top = results_top_3(result_overlap_HT,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7b5980ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10%</th>\n",
       "      <th>20%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>95%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-8.6_61</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-8.7_62</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-8.8_63</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-8.9_64</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  10%  20%  50%  75%  95%\n",
       "size_1-8.6_61     0.0  0.0  0.0  0.0  0.0\n",
       "feeder_3-8.7_62   0.0  0.0  0.0  0.0  0.0\n",
       "gripper_1-8.8_63  1.0  1.0  1.0  1.0  1.0\n",
       "max_Vel_2-8.9_64  1.0  1.0  1.0  1.0  1.0\n",
       "Total             0.5  0.5  0.5  0.5  0.5"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_1top = results_top_1(result_overlap_HT,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8feaf11",
   "metadata": {},
   "source": [
    "### ED - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e8c5bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ED_overlap_1 = run_ED_overlap(folder_path,files,startrow=839,overlap_p=0.10,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_ED_overlap_2 = run_ED_overlap(folder_path,files,startrow=839,overlap_p=0.20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_ED_overlap_3 = run_ED_overlap(folder_path,files,startrow=839,overlap_p=0.50,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_ED_overlap_4 = run_ED_overlap(folder_path,files,startrow=839,overlap_p=0.75,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_ED_overlap_5 = run_ED_overlap(folder_path,files,startrow=839,overlap_p=0.95,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad854d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_overlap_ED = pd.DataFrame()\n",
    "result_overlap_ED = pd.concat([result_ED_overlap_1,result_ED_overlap_2,result_ED_overlap_3,result_ED_overlap_4,result_ED_overlap_5], ignore_index=True)\n",
    "result_overlap_ED['Overlap Percent'] = pd.DataFrame({'ED':['10%','20%','50%','75%','95%']})\n",
    "result_overlap_ED.set_index('Overlap Percent', inplace=True)\n",
    "result_overlap_ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b26d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_overlap_ED,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_overlap_ED,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3c856",
   "metadata": {},
   "source": [
    "### RW - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43587c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RW_overlap_1 = run_RW_overlap(folder_path,files,startrow=839,overlap_p=0.10,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_overlap_2 = run_RW_overlap(folder_path,files,startrow=839,overlap_p=0.20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_overlap_3 = run_RW_overlap(folder_path,files,startrow=839,overlap_p=0.50,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_overlap_4 = run_RW_overlap(folder_path,files,startrow=839,overlap_p=0.75,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_overlap_5 = run_RW_overlap(folder_path,files,startrow=839,overlap_p=0.95,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35564984",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_overlap_RW = pd.DataFrame()\n",
    "result_overlap_RW = pd.concat([result_RW_overlap_1,result_RW_overlap_2,result_RW_overlap_3,result_RW_overlap_4,result_RW_overlap_5], ignore_index=True)\n",
    "result_overlap_RW['Overlap Percent'] = pd.DataFrame({'RW':['10%','20%','50%','75%','95%']})\n",
    "result_overlap_RW.set_index('Overlap Percent', inplace=True)\n",
    "result_overlap_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b28b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_overlap_RW,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_overlap_RW,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f6027",
   "metadata": {},
   "source": [
    "### RCD - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8198d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RCD_overlap_1 = run_RCD_overlap(folder_path,files,startrow=839,overlap_p=0.10,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_overlap_2 = run_RCD_overlap(folder_path,files,startrow=839,overlap_p=0.20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_overlap_3 = run_RCD_overlap(folder_path,files,startrow=839,overlap_p=0.50,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_overlap_4 = run_RCD_overlap(folder_path,files,startrow=839,overlap_p=0.75,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_overlap_5 = run_RCD_overlap(folder_path,files,startrow=839,overlap_p=0.95,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_overlap_RCD = pd.DataFrame()\n",
    "result_overlap_RCD = pd.concat([result_RCD_overlap_1,result_RCD_overlap_2,result_RCD_overlap_3,result_RCD_overlap_4,result_RCD_overlap_5], ignore_index=True)\n",
    "result_overlap_RCD['Overlap Percent'] = pd.DataFrame({'RCD':['10%','20%','50%','75%','95%']})\n",
    "result_overlap_RCD.set_index('Overlap Percent', inplace=True)\n",
    "result_overlap_RCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b819c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_overlap_RCD,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_overlap_RCD,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f9147",
   "metadata": {},
   "source": [
    "## Variation 2 - Normal Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54820a4a",
   "metadata": {},
   "source": [
    "The ED-algorithm requires the same length for the normal and abnormal data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c237377",
   "metadata": {},
   "source": [
    "### HT - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0172d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_HT_normal_size_1 = run_HT_normal_size(folder_path,files,startrow=839,normal_size_p=2.00,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_normal_size_2 = run_HT_normal_size(folder_path,files,startrow=839,normal_size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_normal_size_3 = run_HT_normal_size(folder_path,files,startrow=839,normal_size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_normal_size_4 = run_HT_normal_size(folder_path,files,startrow=839,normal_size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_normal_size_5 = run_HT_normal_size(folder_path,files,startrow=839,normal_size_p=0.2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ae6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_normal_size_HT = pd.DataFrame()\n",
    "result_normal_size_HT = pd.concat([result_HT_normal_size_1,result_HT_normal_size_2,result_HT_normal_size_3,result_HT_normal_size_4,result_HT_normal_size_5], ignore_index=True)\n",
    "result_normal_size_HT['Normal Size Percent'] = pd.DataFrame({'HT':['200%','150%','100%','50%','20%']})\n",
    "result_normal_size_HT.set_index('Normal Size Percent', inplace=True)\n",
    "result_normal_size_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_normal_size_HT,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92579010",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_normal_size_HT,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e130015",
   "metadata": {},
   "source": [
    "### RW - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b550f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RW_normal_size_1 = run_RW_normal_size(folder_path,files,startrow=839,normal_size_p=2.00,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_normal_size_2 = run_RW_normal_size(folder_path,files,startrow=839,normal_size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_normal_size_3 = run_RW_normal_size(folder_path,files,startrow=839,normal_size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_normal_size_4 = run_RW_normal_size(folder_path,files,startrow=839,normal_size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_normal_size_5 = run_RW_normal_size(folder_path,files,startrow=839,normal_size_p=0.2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_normal_size_RW = pd.DataFrame()\n",
    "result_normal_size_RW = pd.concat([result_RW_normal_size_1,result_RW_normal_size_2,result_RW_normal_size_3,result_RW_normal_size_4,result_RW_normal_size_5], ignore_index=True)\n",
    "result_normal_size_RW['Normal Size Percent'] = pd.DataFrame({'RW':['200%','150%','100%','50%','20%']})\n",
    "result_normal_size_RW.set_index('Normal Size Percent', inplace=True)\n",
    "result_normal_size_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_normal_size_RW,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_normal_size_RW,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff6b904",
   "metadata": {},
   "source": [
    "### RCD - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RCD_normal_size_1 = run_RCD_normal_size(folder_path,files,startrow=839,normal_size_p=2.00,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_normal_size_2 = run_RCD_normal_size(folder_path,files,startrow=839,normal_size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_normal_size_3 = run_RCD_normal_size(folder_path,files,startrow=839,normal_size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_normal_size_4 = run_RCD_normal_size(folder_path,files,startrow=839,normal_size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_normal_size_5 = run_RCD_normal_size(folder_path,files,startrow=839,normal_size_p=0.2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acea0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_normal_size_RCD = pd.DataFrame()\n",
    "result_normal_size_RCD = pd.concat([result_RCD_normal_size_1,result_RCD_normal_size_2,result_RCD_normal_size_3,result_RCD_normal_size_4,result_RCD_normal_size_5], ignore_index=True)\n",
    "result_normal_size_RCD['Normal Size Percent'] = pd.DataFrame({'RCD':['200%','150%','100%','50%','20%']})\n",
    "result_normal_size_RCD.set_index('Normal Size Percent', inplace=True)\n",
    "result_normal_size_RCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_normal_size_RCD,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_normal_size_RCD,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7709338",
   "metadata": {},
   "source": [
    "## Variation 3 - Abnormal Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c73ac5",
   "metadata": {},
   "source": [
    "The ED-algorithm requires the same length for the normal and abnormal data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fcbb3",
   "metadata": {},
   "source": [
    "### HT - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_HT_abnormal_size_1 = run_HT_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=2.00,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_abnormal_size_2 = run_HT_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_abnormal_size_3 = run_HT_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_abnormal_size_4 = run_HT_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_abnormal_size_5 = run_HT_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=0.2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_abnormal_size_HT = pd.DataFrame()\n",
    "result_abnormal_size_HT = pd.concat([result_HT_abnormal_size_1,result_HT_abnormal_size_2,result_HT_abnormal_size_3,result_HT_abnormal_size_4,result_HT_abnormal_size_5], ignore_index=True)\n",
    "result_abnormal_size_HT['abnormal Size Percent'] = pd.DataFrame({'HT':['200%','150%','100%','50%','20%']})\n",
    "result_abnormal_size_HT.set_index('abnormal Size Percent', inplace=True)\n",
    "result_abnormal_size_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_abnormal_size_HT,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_abnormal_size_HT,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926c004",
   "metadata": {},
   "source": [
    "### RW - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11042b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RW_abnormal_size_1 = run_RW_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=2.00,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_abnormal_size_2 = run_RW_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_abnormal_size_3 = run_RW_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_abnormal_size_4 = run_RW_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_abnormal_size_5 = run_RW_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=0.2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_abnormal_size_RW = pd.DataFrame()\n",
    "result_abnormal_size_RW = pd.concat([result_RW_abnormal_size_1,result_RW_abnormal_size_2,result_RW_abnormal_size_3,result_RW_abnormal_size_4,result_RW_abnormal_size_5], ignore_index=True)\n",
    "result_abnormal_size_RW['abnormal Size Percent'] = pd.DataFrame({'RW':['200%','150%','100%','50%','20%']})\n",
    "result_abnormal_size_RW.set_index('abnormal Size Percent', inplace=True)\n",
    "result_abnormal_size_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96dce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_abnormal_size_RW,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07538333",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_abnormal_size_RW,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb872b44",
   "metadata": {},
   "source": [
    "### RCD - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RCD_abnormal_size_1 = run_RCD_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=2.00,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_abnormal_size_2 = run_RCD_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_abnormal_size_3 = run_RCD_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_abnormal_size_4 = run_RCD_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_abnormal_size_5 = run_RCD_abnormal_size(folder_path,files,startrow=839,abnormal_size_p=0.2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235196ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_abnormal_size_RCD = pd.DataFrame()\n",
    "result_abnormal_size_RCD = pd.concat([result_RCD_abnormal_size_1,result_RCD_abnormal_size_2,result_RCD_abnormal_size_3,result_RCD_abnormal_size_4,result_RCD_abnormal_size_5], ignore_index=True)\n",
    "result_abnormal_size_RCD['abnormal Size Percent'] = pd.DataFrame({'RCD':['200%','150%','100%','50%','20%']})\n",
    "result_abnormal_size_RCD.set_index('abnormal Size Percent', inplace=True)\n",
    "result_abnormal_size_RCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df026c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_3top = results_top_3(result_abnormal_size_RCD,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4408e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_1top = results_top_1(result_abnormal_size_RCD,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6ece9",
   "metadata": {},
   "source": [
    "## Variation 4 - Causal Graph, adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c38cd1",
   "metadata": {},
   "source": [
    "Only the HT- and RW-Algorithm use an adjacency matrix for training of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885080b",
   "metadata": {},
   "source": [
    "### HT - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_HT_edges_1 = run_HT_edges_delete(folder_path,files,startrow=839,edges_delete_n=20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_edges_2 = run_HT_edges_delete(folder_path,files,startrow=839,edges_delete_n=40,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_edges_3 = run_HT_edges_delete(folder_path,files,startrow=839,edges_delete_n=60,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_edges_4 = run_HT_edges_delete(folder_path,files,startrow=839,edges_delete_n=80,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_edges_5 = run_HT_edges_delete(folder_path,files,startrow=839,edges_delete_n=100,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3982684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_1-1.2_1</th>\n",
       "      <th>feeder_3-1.3_2</th>\n",
       "      <th>gripper_1-1.4_3</th>\n",
       "      <th>max_Vel_2-1.5_4</th>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edges Missing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, rob_1_maxVel, EoL_2_X]</td>\n",
       "      <td>[rob_1_supply, rob_1_maxVel, rob_1_3]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_3]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, EoL_2_X, cam_3_Y]</td>\n",
       "      <td>[rob_1_supply, rob_1_maxVel, rob_1_3]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, cam_3_Y, rob_1_maxVel]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, cam_3_Y, rob_2_maxVel]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_2]</td>\n",
       "      <td>[cam_3_X, cam_3_Y, rob_1_maxVel]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_3]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, cam_3_Y, rob_2_maxVel]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[rob_1_2, rob_1_4, rob_1_vacuum]</td>\n",
       "      <td>[cam_3_X, EoL_2_X, cam_3_Y]</td>\n",
       "      <td>[rob_1_supply, rob_1_3, rob_1_1]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_2, rob_1_4]</td>\n",
       "      <td>[rob_1_2, rob_1_4, rob_1_vacuum]</td>\n",
       "      <td>[cam_3_X, EoL_2_X, cam_3_Y]</td>\n",
       "      <td>[rob_1_supply, rob_1_3, rob_1_1]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_2, rob_1_4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, rob_1_maxVel, rob_2_maxVel]</td>\n",
       "      <td>[rob_1_supply, rob_1_maxVel, rob_1_3]</td>\n",
       "      <td>[rob_2_maxVel, EoL_6_Y, EoL_6_X]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, rob_2_maxVel, EoL_5_X]</td>\n",
       "      <td>[rob_1_supply, rob_1_maxVel, rob_1_3]</td>\n",
       "      <td>[rob_2_maxVel, EoL_6_Y, EoL_6_X]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   size_1-1.2_1  \\\n",
       "Edges Missing                                     \n",
       "20             [rob_1_maxVel, rob_1_3, rob_1_1]   \n",
       "40             [rob_1_maxVel, rob_1_3, rob_1_1]   \n",
       "60             [rob_1_maxVel, rob_1_3, rob_1_2]   \n",
       "80             [rob_1_2, rob_1_4, rob_1_vacuum]   \n",
       "100            [rob_1_maxVel, rob_1_3, rob_1_1]   \n",
       "\n",
       "                                      feeder_3-1.3_2  \\\n",
       "Edges Missing                                          \n",
       "20                  [cam_3_X, rob_1_maxVel, EoL_2_X]   \n",
       "40                  [cam_3_X, cam_3_Y, rob_1_maxVel]   \n",
       "60                  [cam_3_X, cam_3_Y, rob_1_maxVel]   \n",
       "80                       [cam_3_X, EoL_2_X, cam_3_Y]   \n",
       "100            [cam_3_X, rob_1_maxVel, rob_2_maxVel]   \n",
       "\n",
       "                                          gripper_1-1.4_3  \\\n",
       "Edges Missing                                               \n",
       "20                  [rob_1_supply, rob_1_maxVel, rob_1_3]   \n",
       "40             [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "60             [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "80                       [rob_1_supply, rob_1_3, rob_1_1]   \n",
       "100                 [rob_1_supply, rob_1_maxVel, rob_1_3]   \n",
       "\n",
       "                                          max_Vel_2-1.5_4  \\\n",
       "Edges Missing                                               \n",
       "20                  [rob_2_maxVel, rob_1_maxVel, rob_1_3]   \n",
       "40             [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]   \n",
       "60                  [rob_2_maxVel, rob_1_maxVel, rob_1_3]   \n",
       "80                       [rob_2_maxVel, rob_1_2, rob_1_4]   \n",
       "100                      [rob_2_maxVel, EoL_6_Y, EoL_6_X]   \n",
       "\n",
       "                                   size_1-1.6_5  \\\n",
       "Edges Missing                                     \n",
       "20             [rob_1_maxVel, rob_1_3, rob_1_1]   \n",
       "40             [rob_1_maxVel, rob_1_3, rob_1_1]   \n",
       "60             [rob_1_maxVel, rob_1_3, rob_1_1]   \n",
       "80             [rob_1_2, rob_1_4, rob_1_vacuum]   \n",
       "100            [rob_1_maxVel, rob_1_3, rob_1_1]   \n",
       "\n",
       "                                 feeder_3-1.7_6  \\\n",
       "Edges Missing                                     \n",
       "20                  [cam_3_X, EoL_2_X, cam_3_Y]   \n",
       "40             [cam_3_X, cam_3_Y, rob_2_maxVel]   \n",
       "60             [cam_3_X, cam_3_Y, rob_2_maxVel]   \n",
       "80                  [cam_3_X, EoL_2_X, cam_3_Y]   \n",
       "100            [cam_3_X, rob_2_maxVel, EoL_5_X]   \n",
       "\n",
       "                                          gripper_1-1.8_7  \\\n",
       "Edges Missing                                               \n",
       "20                  [rob_1_supply, rob_1_maxVel, rob_1_3]   \n",
       "40             [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "60             [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "80                       [rob_1_supply, rob_1_3, rob_1_1]   \n",
       "100                 [rob_1_supply, rob_1_maxVel, rob_1_3]   \n",
       "\n",
       "                                          max_Vel_2-1.9_8  \n",
       "Edges Missing                                              \n",
       "20                  [rob_2_maxVel, rob_1_maxVel, rob_1_3]  \n",
       "40             [rob_2_maxVel, rob_1_maxVel, rob_1_vacuum]  \n",
       "60                  [rob_2_maxVel, rob_1_maxVel, rob_1_3]  \n",
       "80                       [rob_2_maxVel, rob_1_2, rob_1_4]  \n",
       "100                      [rob_2_maxVel, EoL_6_Y, EoL_6_X]  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_edges_HT = pd.DataFrame()\n",
    "result_edges_HT = pd.concat([result_HT_edges_1,result_HT_edges_2,result_HT_edges_3,result_HT_edges_4,result_HT_edges_5], ignore_index=True)\n",
    "result_edges_HT['Edges Missing'] = pd.DataFrame({'HT':['20','40','60','80','100']})\n",
    "result_edges_HT.set_index('Edges Missing', inplace=True)\n",
    "result_edges_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1f5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   20    40    60    80   100\n",
       "size_1-1.6_5     0.00  0.00  0.00  0.00  0.00\n",
       "feeder_3-1.7_6   1.00  1.00  1.00  1.00  1.00\n",
       "gripper_1-1.8_7  1.00  1.00  1.00  1.00  1.00\n",
       "max_Vel_2-1.9_8  1.00  1.00  1.00  1.00  1.00\n",
       "Total            0.75  0.75  0.75  0.75  0.75"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_3top = results_top_3(result_edges_HT,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737c41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   20    40    60    80   100\n",
       "size_1-1.6_5     0.00  0.00  0.00  0.00  0.00\n",
       "feeder_3-1.7_6   1.00  1.00  1.00  1.00  1.00\n",
       "gripper_1-1.8_7  1.00  1.00  1.00  1.00  1.00\n",
       "max_Vel_2-1.9_8  1.00  1.00  1.00  1.00  1.00\n",
       "Total            0.75  0.75  0.75  0.75  0.75"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_1top = results_top_1(result_edges_HT,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfa42b",
   "metadata": {},
   "source": [
    "### RW - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RW_edges_1 = run_RW_edges_delete(folder_path,files,startrow=839,edges_delete_n=20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_edges_2 = run_RW_edges_delete(folder_path,files,startrow=839,edges_delete_n=40,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_edges_3 = run_RW_edges_delete(folder_path,files,startrow=839,edges_delete_n=60,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_edges_4 = run_RW_edges_delete(folder_path,files,startrow=839,edges_delete_n=80,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_edges_5 = run_RW_edges_delete(folder_path,files,startrow=839,edges_delete_n=100,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912e5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_1-1.2_1</th>\n",
       "      <th>feeder_3-1.3_2</th>\n",
       "      <th>gripper_1-1.4_3</th>\n",
       "      <th>max_Vel_2-1.5_4</th>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edges Missing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[cam_1_Y, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_1_vacuum, rob_2_supply, con_2]</td>\n",
       "      <td>[rob_1_vacuum, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_2_supply, rob_2_maxVel, con_2]</td>\n",
       "      <td>[rob_2_supply, con_2, cam_1_X]</td>\n",
       "      <td>[rob_1_vacuum, rob_2_supply, cam_3_X]</td>\n",
       "      <td>[rob_1_vacuum, rob_2_supply, con_2]</td>\n",
       "      <td>[rob_2_supply, rob_2_maxVel, cam_1_Y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[EoL_1_X, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_1_vacuum, EoL_1_X, rob_2_supply]</td>\n",
       "      <td>[rob_1_vacuum, EoL_1_X, rob_2_supply]</td>\n",
       "      <td>[rob_2_supply, EoL_1_X, rob_2_maxVel]</td>\n",
       "      <td>[EoL_1_X, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_1_vacuum, rob_2_supply, EoL_1_X]</td>\n",
       "      <td>[rob_1_vacuum, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[EoL_1_X, rob_2_supply, rob_2_maxVel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[EoL_1_Y, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_2_supply, EoL_1_Y, cam_1_X]</td>\n",
       "      <td>[rob_2_supply, EoL_1_Y, cam_1_X]</td>\n",
       "      <td>[rob_2_supply, rob_2_maxVel, EoL_1_Y]</td>\n",
       "      <td>[rob_2_supply, cam_1_X, EoL_1_Y]</td>\n",
       "      <td>[rob_2_supply, EoL_1_Y, rob_2_maxVel]</td>\n",
       "      <td>[rob_2_supply, EoL_1_Y, rob_1_vacuum]</td>\n",
       "      <td>[EoL_1_Y, rob_2_supply, rob_2_maxVel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[rob_2_vacuum, EoL_1_X, cam_1_X]</td>\n",
       "      <td>[rob_2_vacuum, EoL_1_X, cam_3_X]</td>\n",
       "      <td>[rob_2_vacuum, EoL_1_X, cam_1_X]</td>\n",
       "      <td>[EoL_1_X, rob_2_vacuum, cam_1_X]</td>\n",
       "      <td>[rob_2_vacuum, EoL_1_X, cam_1_X]</td>\n",
       "      <td>[rob_2_vacuum, EoL_1_X, cam_1_X]</td>\n",
       "      <td>[rob_2_vacuum, EoL_1_X, cam_1_X]</td>\n",
       "      <td>[EoL_1_X, rob_2_vacuum, cam_1_X]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[rob_2_vacuum, cam_1_X, cam_2_X]</td>\n",
       "      <td>[rob_2_vacuum, cam_1_X, cam_2_X]</td>\n",
       "      <td>[rob_2_vacuum, cam_1_X, cam_2_X]</td>\n",
       "      <td>[rob_2_vacuum, con_2, cam_1_X]</td>\n",
       "      <td>[rob_2_vacuum, cam_1_X, cam_2_X]</td>\n",
       "      <td>[rob_2_vacuum, cam_1_X, cam_2_X]</td>\n",
       "      <td>[rob_2_vacuum, cam_1_X, cam_2_X]</td>\n",
       "      <td>[rob_2_vacuum, cam_1_X, cam_2_X]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   size_1-1.2_1  \\\n",
       "Edges Missing                                     \n",
       "20             [cam_1_Y, rob_2_supply, cam_1_X]   \n",
       "40             [EoL_1_X, rob_2_supply, cam_1_X]   \n",
       "60             [EoL_1_Y, rob_2_supply, cam_1_X]   \n",
       "80             [rob_2_vacuum, EoL_1_X, cam_1_X]   \n",
       "100            [rob_2_vacuum, cam_1_X, cam_2_X]   \n",
       "\n",
       "                                      feeder_3-1.3_2  \\\n",
       "Edges Missing                                          \n",
       "20               [rob_1_vacuum, rob_2_supply, con_2]   \n",
       "40             [rob_1_vacuum, EoL_1_X, rob_2_supply]   \n",
       "60                  [rob_2_supply, EoL_1_Y, cam_1_X]   \n",
       "80                  [rob_2_vacuum, EoL_1_X, cam_3_X]   \n",
       "100                 [rob_2_vacuum, cam_1_X, cam_2_X]   \n",
       "\n",
       "                                     gripper_1-1.4_3  \\\n",
       "Edges Missing                                          \n",
       "20             [rob_1_vacuum, rob_2_supply, cam_1_X]   \n",
       "40             [rob_1_vacuum, EoL_1_X, rob_2_supply]   \n",
       "60                  [rob_2_supply, EoL_1_Y, cam_1_X]   \n",
       "80                  [rob_2_vacuum, EoL_1_X, cam_1_X]   \n",
       "100                 [rob_2_vacuum, cam_1_X, cam_2_X]   \n",
       "\n",
       "                                     max_Vel_2-1.5_4  \\\n",
       "Edges Missing                                          \n",
       "20               [rob_2_supply, rob_2_maxVel, con_2]   \n",
       "40             [rob_2_supply, EoL_1_X, rob_2_maxVel]   \n",
       "60             [rob_2_supply, rob_2_maxVel, EoL_1_Y]   \n",
       "80                  [EoL_1_X, rob_2_vacuum, cam_1_X]   \n",
       "100                   [rob_2_vacuum, con_2, cam_1_X]   \n",
       "\n",
       "                                   size_1-1.6_5  \\\n",
       "Edges Missing                                     \n",
       "20               [rob_2_supply, con_2, cam_1_X]   \n",
       "40             [EoL_1_X, rob_2_supply, cam_1_X]   \n",
       "60             [rob_2_supply, cam_1_X, EoL_1_Y]   \n",
       "80             [rob_2_vacuum, EoL_1_X, cam_1_X]   \n",
       "100            [rob_2_vacuum, cam_1_X, cam_2_X]   \n",
       "\n",
       "                                      feeder_3-1.7_6  \\\n",
       "Edges Missing                                          \n",
       "20             [rob_1_vacuum, rob_2_supply, cam_3_X]   \n",
       "40             [rob_1_vacuum, rob_2_supply, EoL_1_X]   \n",
       "60             [rob_2_supply, EoL_1_Y, rob_2_maxVel]   \n",
       "80                  [rob_2_vacuum, EoL_1_X, cam_1_X]   \n",
       "100                 [rob_2_vacuum, cam_1_X, cam_2_X]   \n",
       "\n",
       "                                     gripper_1-1.8_7  \\\n",
       "Edges Missing                                          \n",
       "20               [rob_1_vacuum, rob_2_supply, con_2]   \n",
       "40             [rob_1_vacuum, rob_2_supply, cam_1_X]   \n",
       "60             [rob_2_supply, EoL_1_Y, rob_1_vacuum]   \n",
       "80                  [rob_2_vacuum, EoL_1_X, cam_1_X]   \n",
       "100                 [rob_2_vacuum, cam_1_X, cam_2_X]   \n",
       "\n",
       "                                     max_Vel_2-1.9_8  \n",
       "Edges Missing                                         \n",
       "20             [rob_2_supply, rob_2_maxVel, cam_1_Y]  \n",
       "40             [EoL_1_X, rob_2_supply, rob_2_maxVel]  \n",
       "60             [EoL_1_Y, rob_2_supply, rob_2_maxVel]  \n",
       "80                  [EoL_1_X, rob_2_vacuum, cam_1_X]  \n",
       "100                 [rob_2_vacuum, cam_1_X, cam_2_X]  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_edges_RW = pd.DataFrame()\n",
    "result_edges_RW = pd.concat([result_RW_edges_1,result_RW_edges_2,result_RW_edges_3,result_RW_edges_4,result_RW_edges_5], ignore_index=True)\n",
    "result_edges_RW['Edges Missing'] = pd.DataFrame({'RW':['20','40','60','80','100']})\n",
    "result_edges_RW.set_index('Edges Missing', inplace=True)\n",
    "result_edges_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd3c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    20   40   60     80   100\n",
       "size_1-1.6_5     1.000  1.0  1.0  1.000  1.00\n",
       "feeder_3-1.7_6   1.000  0.0  0.0  0.000  0.00\n",
       "gripper_1-1.8_7  0.000  0.0  0.0  0.000  0.00\n",
       "max_Vel_2-1.9_8  1.000  1.0  1.0  0.000  0.00\n",
       "Total            0.625  0.5  0.5  0.375  0.25"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_3top = results_top_3(result_edges_RW,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63b68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    20   40   60   80  100\n",
       "size_1-1.6_5     0.000  0.0  0.0  0.0  0.0\n",
       "feeder_3-1.7_6   0.000  0.0  0.0  0.0  0.0\n",
       "gripper_1-1.8_7  0.000  0.0  0.0  0.0  0.0\n",
       "max_Vel_2-1.9_8  0.000  0.0  0.0  0.0  0.0\n",
       "Total            0.125  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_1top = results_top_1(result_edges_RW,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04deb0",
   "metadata": {},
   "source": [
    "## Variation 5 - Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b2226",
   "metadata": {},
   "source": [
    "### HT -Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1052, 36) (1052, 36)\n",
      "(1052, 36) (1052, 36)\n",
      "(1052, 36) (1052, 36)\n",
      "(1052, 36) (1052, 36)\n",
      "(1052, 36) (1052, 36)\n",
      "(1052, 36) (1052, 36)\n",
      "(1052, 36) (1052, 36)\n",
      "(1052, 36) (1052, 36)\n",
      "(789, 36) (789, 36)\n",
      "(789, 36) (789, 36)\n",
      "(789, 36) (789, 36)\n",
      "(789, 36) (789, 36)\n",
      "(789, 36) (789, 36)\n",
      "(789, 36) (789, 36)\n",
      "(789, 36) (789, 36)\n",
      "(789, 36) (789, 36)\n",
      "(526, 36) (526, 36)\n",
      "(526, 36) (526, 36)\n",
      "(526, 36) (526, 36)\n",
      "(526, 36) (526, 36)\n",
      "(526, 36) (526, 36)\n",
      "(526, 36) (526, 36)\n",
      "(526, 36) (526, 36)\n",
      "(526, 36) (526, 36)\n",
      "(263, 36) (263, 36)\n",
      "(263, 36) (263, 36)\n",
      "(263, 36) (263, 36)\n",
      "(263, 36) (263, 36)\n",
      "(263, 36) (263, 36)\n",
      "(263, 36) (263, 36)\n",
      "(263, 36) (263, 36)\n",
      "(263, 36) (263, 36)\n",
      "(105, 36) (105, 36)\n",
      "(105, 36) (105, 36)\n",
      "(105, 36) (105, 36)\n",
      "(105, 36) (105, 36)\n",
      "(105, 36) (105, 36)\n",
      "(105, 36) (105, 36)\n",
      "(105, 36) (105, 36)\n",
      "(105, 36) (105, 36)\n"
     ]
    }
   ],
   "source": [
    "result_HT_size_1 = run_HT_size(folder_path,files,startrow=839,size_p=2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_size_2 = run_HT_size(folder_path,files,startrow=839,size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_size_3 = run_HT_size(folder_path,files,startrow=839,size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_size_4 = run_HT_size(folder_path,files,startrow=839,size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_HT_size_5 = run_HT_size(folder_path,files,startrow=839,size_p=0.2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6e28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_1-1.2_1</th>\n",
       "      <th>feeder_3-1.3_2</th>\n",
       "      <th>gripper_1-1.4_3</th>\n",
       "      <th>max_Vel_2-1.5_4</th>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200%</th>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, rob_1_maxVel, EoL_2_X]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_3]</td>\n",
       "      <td>[rob_1_maxVel, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, EoL_2_X, cam_3_Y]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, rob_1_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150%</th>\n",
       "      <td>[rob_1_maxVel, con_3, rob_1_3]</td>\n",
       "      <td>[cam_3_X, rob_1_maxVel, con_3]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, con_3]</td>\n",
       "      <td>[rob_1_maxVel, con_3, rob_1_3]</td>\n",
       "      <td>[cam_3_X, con_3, EoL_2_X]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, con_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>[con_3, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, con_3, rob_1_2]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_maxVel]</td>\n",
       "      <td>[rob_2_maxVel, con_3, rob_1_vacuum]</td>\n",
       "      <td>[con_3, rob_1_3, rob_1_1]</td>\n",
       "      <td>[cam_3_X, con_3, rob_1_2]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, con_3]</td>\n",
       "      <td>[rob_2_maxVel, rob_1_maxVel, con_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_1]</td>\n",
       "      <td>[rob_1_4, cam_3_Y, cam_2_Y]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_1]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_1]</td>\n",
       "      <td>[rob_1_3, rob_1_1, rob_1_2]</td>\n",
       "      <td>[rob_1_4, cam_3_Y, rob_1_3]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_1]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>[cam_1_X, cam_1_Y, rob_2_4]</td>\n",
       "      <td>[score, rob_2_maxVel, cam_1_X]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_1]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_1, rob_2_3]</td>\n",
       "      <td>[cam_1_X, cam_1_Y, rob_2_4]</td>\n",
       "      <td>[score, rob_2_maxVel, rob_1_maxVel]</td>\n",
       "      <td>[rob_1_supply, rob_1_vacuum, rob_1_1]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_1, rob_2_3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               size_1-1.2_1                    feeder_3-1.3_2  \\\n",
       "Data Size                                                                       \n",
       "200%       [rob_1_maxVel, rob_1_3, rob_1_1]  [cam_3_X, rob_1_maxVel, EoL_2_X]   \n",
       "150%         [rob_1_maxVel, con_3, rob_1_3]    [cam_3_X, rob_1_maxVel, con_3]   \n",
       "100%              [con_3, rob_1_3, rob_1_1]         [cam_3_X, con_3, rob_1_2]   \n",
       "50%             [rob_1_4, rob_1_3, rob_1_1]       [rob_1_4, cam_3_Y, cam_2_Y]   \n",
       "20%             [cam_1_X, cam_1_Y, rob_2_4]    [score, rob_2_maxVel, cam_1_X]   \n",
       "\n",
       "                                      gripper_1-1.4_3  \\\n",
       "Data Size                                               \n",
       "200%       [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "150%       [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "100%       [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "50%                       [rob_1_4, rob_1_3, rob_1_1]   \n",
       "20%             [rob_1_supply, rob_1_vacuum, rob_1_1]   \n",
       "\n",
       "                                 max_Vel_2-1.5_4  \\\n",
       "Data Size                                          \n",
       "200%       [rob_2_maxVel, rob_1_maxVel, rob_1_3]   \n",
       "150%         [rob_2_maxVel, rob_1_maxVel, con_3]   \n",
       "100%         [rob_2_maxVel, con_3, rob_1_vacuum]   \n",
       "50%                  [rob_1_4, rob_1_3, rob_1_1]   \n",
       "20%             [rob_2_maxVel, rob_2_1, rob_2_3]   \n",
       "\n",
       "                               size_1-1.6_5  \\\n",
       "Data Size                                     \n",
       "200%       [rob_1_maxVel, rob_1_3, rob_1_1]   \n",
       "150%         [rob_1_maxVel, con_3, rob_1_3]   \n",
       "100%              [con_3, rob_1_3, rob_1_1]   \n",
       "50%             [rob_1_3, rob_1_1, rob_1_2]   \n",
       "20%             [cam_1_X, cam_1_Y, rob_2_4]   \n",
       "\n",
       "                                feeder_3-1.7_6  \\\n",
       "Data Size                                        \n",
       "200%               [cam_3_X, EoL_2_X, cam_3_Y]   \n",
       "150%                 [cam_3_X, con_3, EoL_2_X]   \n",
       "100%                 [cam_3_X, con_3, rob_1_2]   \n",
       "50%                [rob_1_4, cam_3_Y, rob_1_3]   \n",
       "20%        [score, rob_2_maxVel, rob_1_maxVel]   \n",
       "\n",
       "                                      gripper_1-1.8_7  \\\n",
       "Data Size                                               \n",
       "200%       [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "150%       [rob_1_supply, rob_1_vacuum, rob_1_maxVel]   \n",
       "100%              [rob_1_supply, rob_1_vacuum, con_3]   \n",
       "50%                       [rob_1_4, rob_1_3, rob_1_1]   \n",
       "20%             [rob_1_supply, rob_1_vacuum, rob_1_1]   \n",
       "\n",
       "                                 max_Vel_2-1.9_8  \n",
       "Data Size                                         \n",
       "200%       [rob_2_maxVel, rob_1_maxVel, rob_1_3]  \n",
       "150%         [rob_2_maxVel, rob_1_maxVel, con_3]  \n",
       "100%         [rob_2_maxVel, rob_1_maxVel, con_3]  \n",
       "50%                  [rob_1_4, rob_1_3, rob_1_1]  \n",
       "20%             [rob_2_maxVel, rob_2_1, rob_2_3]  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_size_HT = pd.DataFrame()\n",
    "result_size_HT = pd.concat([result_HT_size_1,result_HT_size_2,result_HT_size_3,result_HT_size_4,result_HT_size_5], ignore_index=True)\n",
    "result_size_HT['Data Size'] = pd.DataFrame({'HT':['200%','150%','100%','50%','20%']})\n",
    "result_size_HT.set_index('Data Size', inplace=True)\n",
    "result_size_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac14219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200%</th>\n",
       "      <th>150%</th>\n",
       "      <th>100%</th>\n",
       "      <th>50%</th>\n",
       "      <th>20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 200%  150%  100%   50%   20%\n",
       "size_1-1.6_5     0.00  0.00  0.00  0.00  1.00\n",
       "feeder_3-1.7_6   1.00  1.00  1.00  1.00  0.00\n",
       "gripper_1-1.8_7  1.00  1.00  1.00  0.00  1.00\n",
       "max_Vel_2-1.9_8  1.00  1.00  1.00  0.00  1.00\n",
       "Total            0.75  0.75  0.75  0.25  0.75"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_3top = results_top_3(result_size_HT,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731942f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200%</th>\n",
       "      <th>150%</th>\n",
       "      <th>100%</th>\n",
       "      <th>50%</th>\n",
       "      <th>20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 200%  150%  100%  50%   20%\n",
       "size_1-1.6_5     0.00  0.00  0.00  0.0  1.00\n",
       "feeder_3-1.7_6   1.00  1.00  1.00  0.0  0.00\n",
       "gripper_1-1.8_7  1.00  1.00  1.00  0.0  1.00\n",
       "max_Vel_2-1.9_8  1.00  1.00  1.00  0.0  1.00\n",
       "Total            0.75  0.75  0.75  0.0  0.75"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_1top = results_top_1(result_size_HT,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055b8ec",
   "metadata": {},
   "source": [
    "### ED - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ED_size_1 = run_ED_size(folder_path,files,startrow=839,size_p=2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_ED_size_2 = run_ED_size(folder_path,files,startrow=839,size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_ED_size_3 = run_ED_size(folder_path,files,startrow=839,size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_ED_size_4 = run_ED_size(folder_path,files,startrow=839,size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_ED_size_5 = run_ED_size(folder_path,files,startrow=839,size_p=0.20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec65c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_1-1.2_1</th>\n",
       "      <th>feeder_3-1.3_2</th>\n",
       "      <th>gripper_1-1.4_3</th>\n",
       "      <th>max_Vel_2-1.5_4</th>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200%</th>\n",
       "      <td>[cam_3_Y, EoL_1_X, rob_1_vacuum]</td>\n",
       "      <td>[cam_2_X, EoL_1_Y, con_1]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_2]</td>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "      <td>[cam_3_Y, cam_2_X, rob_1_vacuum]</td>\n",
       "      <td>[cam_2_Y, con_1, EoL_1_Y]</td>\n",
       "      <td>[cam_3_Y, rob_1_4, rob_1_3]</td>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150%</th>\n",
       "      <td>[EoL_1_X, cam_3_Y, rob_1_vacuum]</td>\n",
       "      <td>[cam_2_X, EoL_1_Y, cam_2_Y]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_2]</td>\n",
       "      <td>[EoL_4_X, rob_1_vacuum, rob_1_4]</td>\n",
       "      <td>[cam_3_Y, cam_2_X, rob_1_vacuum]</td>\n",
       "      <td>[cam_2_Y, cam_2_X, con_1]</td>\n",
       "      <td>[cam_3_Y, rob_1_4, rob_1_3]</td>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>[cam_2_X, EoL_1_X, cam_3_Y]</td>\n",
       "      <td>[cam_2_X, cam_2_Y, EoL_1_Y]</td>\n",
       "      <td>[cam_2_Y, rob_1_4, rob_1_3]</td>\n",
       "      <td>[EoL_4_X, cam_3_Y, cam_2_Y]</td>\n",
       "      <td>[cam_2_X, cam_3_Y, cam_2_Y]</td>\n",
       "      <td>[cam_2_X, cam_2_Y, EoL_1_Y]</td>\n",
       "      <td>[cam_3_Y, rob_1_4, rob_1_3]</td>\n",
       "      <td>[cam_3_Y, rob_1_vacuum, rob_1_4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>[EoL_2_X, EoL_2_Y, cam_3_Y]</td>\n",
       "      <td>[cam_2_Y, cam_2_X, con_1]</td>\n",
       "      <td>[cam_3_Y, cam_2_Y, EoL_1_Y]</td>\n",
       "      <td>[EoL_2_X, EoL_3_X, EoL_4_X]</td>\n",
       "      <td>[EoL_2_X, cam_3_Y, cam_2_X]</td>\n",
       "      <td>[cam_2_Y, cam_2_X, EoL_1_Y]</td>\n",
       "      <td>[cam_3_Y, cam_2_Y, rob_1_4]</td>\n",
       "      <td>[EoL_3_X, EoL_4_X, EoL_5_X]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>[rob_1_2, rob_1_1, rob_1_4]</td>\n",
       "      <td>[con_1, rob_1_supply, rob_1_maxVel]</td>\n",
       "      <td>[rob_1_4, rob_1_3, rob_1_2]</td>\n",
       "      <td>[rob_1_2, rob_1_4, rob_1_3]</td>\n",
       "      <td>[rob_1_vacuum, rob_1_4, rob_1_3]</td>\n",
       "      <td>[cam_1_Y, cam_1_X, con_1]</td>\n",
       "      <td>[rob_1_2, rob_1_4, rob_1_3]</td>\n",
       "      <td>[rob_1_2, rob_1_4, rob_1_3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               size_1-1.2_1  \\\n",
       "Data Size                                     \n",
       "200%       [cam_3_Y, EoL_1_X, rob_1_vacuum]   \n",
       "150%       [EoL_1_X, cam_3_Y, rob_1_vacuum]   \n",
       "100%            [cam_2_X, EoL_1_X, cam_3_Y]   \n",
       "50%             [EoL_2_X, EoL_2_Y, cam_3_Y]   \n",
       "20%             [rob_1_2, rob_1_1, rob_1_4]   \n",
       "\n",
       "                                feeder_3-1.3_2              gripper_1-1.4_3  \\\n",
       "Data Size                                                                     \n",
       "200%                 [cam_2_X, EoL_1_Y, con_1]  [rob_1_4, rob_1_3, rob_1_2]   \n",
       "150%               [cam_2_X, EoL_1_Y, cam_2_Y]  [rob_1_4, rob_1_3, rob_1_2]   \n",
       "100%               [cam_2_X, cam_2_Y, EoL_1_Y]  [cam_2_Y, rob_1_4, rob_1_3]   \n",
       "50%                  [cam_2_Y, cam_2_X, con_1]  [cam_3_Y, cam_2_Y, EoL_1_Y]   \n",
       "20%        [con_1, rob_1_supply, rob_1_maxVel]  [rob_1_4, rob_1_3, rob_1_2]   \n",
       "\n",
       "                            max_Vel_2-1.5_4                      size_1-1.6_5  \\\n",
       "Data Size                                                                       \n",
       "200%       [rob_1_vacuum, rob_1_4, rob_1_3]  [cam_3_Y, cam_2_X, rob_1_vacuum]   \n",
       "150%       [EoL_4_X, rob_1_vacuum, rob_1_4]  [cam_3_Y, cam_2_X, rob_1_vacuum]   \n",
       "100%            [EoL_4_X, cam_3_Y, cam_2_Y]       [cam_2_X, cam_3_Y, cam_2_Y]   \n",
       "50%             [EoL_2_X, EoL_3_X, EoL_4_X]       [EoL_2_X, cam_3_Y, cam_2_X]   \n",
       "20%             [rob_1_2, rob_1_4, rob_1_3]  [rob_1_vacuum, rob_1_4, rob_1_3]   \n",
       "\n",
       "                        feeder_3-1.7_6              gripper_1-1.8_7  \\\n",
       "Data Size                                                             \n",
       "200%         [cam_2_Y, con_1, EoL_1_Y]  [cam_3_Y, rob_1_4, rob_1_3]   \n",
       "150%         [cam_2_Y, cam_2_X, con_1]  [cam_3_Y, rob_1_4, rob_1_3]   \n",
       "100%       [cam_2_X, cam_2_Y, EoL_1_Y]  [cam_3_Y, rob_1_4, rob_1_3]   \n",
       "50%        [cam_2_Y, cam_2_X, EoL_1_Y]  [cam_3_Y, cam_2_Y, rob_1_4]   \n",
       "20%          [cam_1_Y, cam_1_X, con_1]  [rob_1_2, rob_1_4, rob_1_3]   \n",
       "\n",
       "                            max_Vel_2-1.9_8  \n",
       "Data Size                                    \n",
       "200%       [rob_1_vacuum, rob_1_4, rob_1_3]  \n",
       "150%       [rob_1_vacuum, rob_1_4, rob_1_3]  \n",
       "100%       [cam_3_Y, rob_1_vacuum, rob_1_4]  \n",
       "50%             [EoL_3_X, EoL_4_X, EoL_5_X]  \n",
       "20%             [rob_1_2, rob_1_4, rob_1_3]  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_size_ED = pd.DataFrame()\n",
    "result_size_ED = pd.concat([result_ED_size_1,result_ED_size_2,result_ED_size_3,result_ED_size_4,result_ED_size_5], ignore_index=True)\n",
    "result_size_ED['Data Size'] = pd.DataFrame({'ED':['200%','150%','100%','50%','20%']})\n",
    "result_size_ED.set_index('Data Size', inplace=True)\n",
    "result_size_ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f190b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200%</th>\n",
       "      <th>150%</th>\n",
       "      <th>100%</th>\n",
       "      <th>50%</th>\n",
       "      <th>20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 200%  150%  100%  50%  20%\n",
       "size_1-1.6_5      0.0   0.0   0.0  0.0  0.0\n",
       "feeder_3-1.7_6    0.0   0.0   0.0  0.0  0.0\n",
       "gripper_1-1.8_7   0.0   0.0   0.0  0.0  0.0\n",
       "max_Vel_2-1.9_8   0.0   0.0   0.0  0.0  0.0\n",
       "Total             0.0   0.0   0.0  0.0  0.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_3top = results_top_3(result_size_ED,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2b6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200%</th>\n",
       "      <th>150%</th>\n",
       "      <th>100%</th>\n",
       "      <th>50%</th>\n",
       "      <th>20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 200%  150%  100%  50%  20%\n",
       "size_1-1.6_5      0.0   0.0   0.0  0.0  0.0\n",
       "feeder_3-1.7_6    0.0   0.0   0.0  0.0  0.0\n",
       "gripper_1-1.8_7   0.0   0.0   0.0  0.0  0.0\n",
       "max_Vel_2-1.9_8   0.0   0.0   0.0  0.0  0.0\n",
       "Total             0.0   0.0   0.0  0.0  0.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_1top = results_top_1(result_size_ED,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74bc8a9",
   "metadata": {},
   "source": [
    "### RW - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce213e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RW_size_1 = run_RW_size(folder_path,files,startrow=839,size_p=2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_size_2 = run_RW_size(folder_path,files,startrow=839,size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_size_3 = run_RW_size(folder_path,files,startrow=839,size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_size_4 = run_RW_size(folder_path,files,startrow=839,size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RW_size_5 = run_RW_size(folder_path,files,startrow=839,size_p=0.20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876ded6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_1-1.2_1</th>\n",
       "      <th>feeder_3-1.3_2</th>\n",
       "      <th>gripper_1-1.4_3</th>\n",
       "      <th>max_Vel_2-1.5_4</th>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200%</th>\n",
       "      <td>[rob_2_supply, cam_1_Y, cam_1_X]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_2_supply, rob_2_maxVel, cam_1_Y]</td>\n",
       "      <td>[rob_2_supply, cam_1_X, cam_1_Y]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_1_supply, cam_1_Y, cam_1_X]</td>\n",
       "      <td>[rob_2_supply, rob_2_maxVel, cam_1_Y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150%</th>\n",
       "      <td>[cam_1_X, cam_1_Y, rob_2_supply]</td>\n",
       "      <td>[rob_2_supply, rob_1_supply, cam_1_X]</td>\n",
       "      <td>[rob_1_supply, cam_1_X, cam_1_Y]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[cam_1_Y, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_2_supply, rob_1_supply, cam_1_X]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_Y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>[cam_1_X, cam_1_Y, rob_2_supply]</td>\n",
       "      <td>[rob_2_supply, cam_3_Y, cam_3_X]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, cam_1_X]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_Y]</td>\n",
       "      <td>[rob_2_supply, cam_1_X, cam_1_Y]</td>\n",
       "      <td>[rob_2_supply, rob_1_supply, cam_3_X]</td>\n",
       "      <td>[rob_1_supply, cam_1_Y, rob_2_supply]</td>\n",
       "      <td>[rob_2_maxVel, rob_2_supply, cam_1_Y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>[cam_1_X, cam_1_Y, rob_2_supply]</td>\n",
       "      <td>[rob_2_supply, cam_1_X, rob_1_supply]</td>\n",
       "      <td>[rob_1_supply, cam_1_X, rob_2_supply]</td>\n",
       "      <td>[rob_2_maxVel, cam_1_X, rob_2_supply]</td>\n",
       "      <td>[cam_1_Y, cam_1_X, con_2]</td>\n",
       "      <td>[rob_2_supply, rob_1_supply, cam_1_Y]</td>\n",
       "      <td>[rob_1_supply, cam_1_X, cam_1_Y]</td>\n",
       "      <td>[rob_2_maxVel, cam_1_Y, rob_2_supply]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>[cam_1_X, cam_1_Y, rob_2_supply]</td>\n",
       "      <td>[rob_1_supply, cam_1_X, rob_2_supply]</td>\n",
       "      <td>[rob_1_supply, cam_1_X, rob_2_supply]</td>\n",
       "      <td>[rob_2_maxVel, cam_1_Y, cam_1_X]</td>\n",
       "      <td>[cam_1_Y, cam_1_X, rob_2_supply]</td>\n",
       "      <td>[rob_1_supply, rob_2_supply, rob_2_maxVel]</td>\n",
       "      <td>[rob_1_supply, cam_1_Y, rob_2_supply]</td>\n",
       "      <td>[rob_2_maxVel, cam_1_X, cam_1_Y]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               size_1-1.2_1  \\\n",
       "Data Size                                     \n",
       "200%       [rob_2_supply, cam_1_Y, cam_1_X]   \n",
       "150%       [cam_1_X, cam_1_Y, rob_2_supply]   \n",
       "100%       [cam_1_X, cam_1_Y, rob_2_supply]   \n",
       "50%        [cam_1_X, cam_1_Y, rob_2_supply]   \n",
       "20%        [cam_1_X, cam_1_Y, rob_2_supply]   \n",
       "\n",
       "                                  feeder_3-1.3_2  \\\n",
       "Data Size                                          \n",
       "200%       [rob_1_supply, rob_2_supply, cam_1_X]   \n",
       "150%       [rob_2_supply, rob_1_supply, cam_1_X]   \n",
       "100%            [rob_2_supply, cam_3_Y, cam_3_X]   \n",
       "50%        [rob_2_supply, cam_1_X, rob_1_supply]   \n",
       "20%        [rob_1_supply, cam_1_X, rob_2_supply]   \n",
       "\n",
       "                                 gripper_1-1.4_3  \\\n",
       "Data Size                                          \n",
       "200%       [rob_1_supply, rob_2_supply, cam_1_Y]   \n",
       "150%            [rob_1_supply, cam_1_X, cam_1_Y]   \n",
       "100%       [rob_1_supply, rob_2_supply, cam_1_X]   \n",
       "50%        [rob_1_supply, cam_1_X, rob_2_supply]   \n",
       "20%        [rob_1_supply, cam_1_X, rob_2_supply]   \n",
       "\n",
       "                                 max_Vel_2-1.5_4  \\\n",
       "Data Size                                          \n",
       "200%       [rob_2_supply, rob_2_maxVel, cam_1_Y]   \n",
       "150%       [rob_2_maxVel, rob_2_supply, cam_1_Y]   \n",
       "100%       [rob_2_maxVel, rob_2_supply, cam_1_Y]   \n",
       "50%        [rob_2_maxVel, cam_1_X, rob_2_supply]   \n",
       "20%             [rob_2_maxVel, cam_1_Y, cam_1_X]   \n",
       "\n",
       "                               size_1-1.6_5  \\\n",
       "Data Size                                     \n",
       "200%       [rob_2_supply, cam_1_X, cam_1_Y]   \n",
       "150%       [cam_1_Y, rob_2_supply, cam_1_X]   \n",
       "100%       [rob_2_supply, cam_1_X, cam_1_Y]   \n",
       "50%               [cam_1_Y, cam_1_X, con_2]   \n",
       "20%        [cam_1_Y, cam_1_X, rob_2_supply]   \n",
       "\n",
       "                                       feeder_3-1.7_6  \\\n",
       "Data Size                                               \n",
       "200%            [rob_1_supply, rob_2_supply, cam_1_Y]   \n",
       "150%            [rob_2_supply, rob_1_supply, cam_1_X]   \n",
       "100%            [rob_2_supply, rob_1_supply, cam_3_X]   \n",
       "50%             [rob_2_supply, rob_1_supply, cam_1_Y]   \n",
       "20%        [rob_1_supply, rob_2_supply, rob_2_maxVel]   \n",
       "\n",
       "                                 gripper_1-1.8_7  \\\n",
       "Data Size                                          \n",
       "200%            [rob_1_supply, cam_1_Y, cam_1_X]   \n",
       "150%       [rob_1_supply, rob_2_supply, cam_1_Y]   \n",
       "100%       [rob_1_supply, cam_1_Y, rob_2_supply]   \n",
       "50%             [rob_1_supply, cam_1_X, cam_1_Y]   \n",
       "20%        [rob_1_supply, cam_1_Y, rob_2_supply]   \n",
       "\n",
       "                                 max_Vel_2-1.9_8  \n",
       "Data Size                                         \n",
       "200%       [rob_2_supply, rob_2_maxVel, cam_1_Y]  \n",
       "150%       [rob_2_maxVel, rob_2_supply, cam_1_Y]  \n",
       "100%       [rob_2_maxVel, rob_2_supply, cam_1_Y]  \n",
       "50%        [rob_2_maxVel, cam_1_Y, rob_2_supply]  \n",
       "20%             [rob_2_maxVel, cam_1_X, cam_1_Y]  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_size_RW = pd.DataFrame()\n",
    "result_size_RW = pd.concat([result_RW_size_1,result_RW_size_2,result_RW_size_3,result_RW_size_4,result_RW_size_5], ignore_index=True)\n",
    "result_size_RW['Data Size'] = pd.DataFrame({'RW':['200%','150%','100%','50%','20%']})\n",
    "result_size_RW.set_index('Data Size', inplace=True)\n",
    "result_size_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa48db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200%</th>\n",
       "      <th>150%</th>\n",
       "      <th>100%</th>\n",
       "      <th>50%</th>\n",
       "      <th>20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 200%  150%  100%   50%   20%\n",
       "size_1-1.6_5     1.00  1.00   1.0  1.00  1.00\n",
       "feeder_3-1.7_6   0.00  0.00   1.0  0.00  0.00\n",
       "gripper_1-1.8_7  1.00  1.00   1.0  1.00  1.00\n",
       "max_Vel_2-1.9_8  1.00  1.00   1.0  1.00  1.00\n",
       "Total            0.75  0.75   1.0  0.75  0.75"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_3top = results_top_3(result_size_RW,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63da619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200%</th>\n",
       "      <th>150%</th>\n",
       "      <th>100%</th>\n",
       "      <th>50%</th>\n",
       "      <th>20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 200%  150%   100%   50%   20%\n",
       "size_1-1.6_5     0.00  1.00  0.000  1.00  1.00\n",
       "feeder_3-1.7_6   0.00  0.00  0.000  0.00  0.00\n",
       "gripper_1-1.8_7  1.00  1.00  1.000  1.00  1.00\n",
       "max_Vel_2-1.9_8  0.00  1.00  1.000  1.00  1.00\n",
       "Total            0.25  0.75  0.625  0.75  0.75"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_1top = results_top_1(result_size_RW,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943cf714",
   "metadata": {},
   "source": [
    "### RCD - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19870e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_RCD_size_1 = run_RCD_size(folder_path,files,startrow=839,size_p=2,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_size_2 = run_RCD_size(folder_path,files,startrow=839,size_p=1.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_size_3 = run_RCD_size(folder_path,files,startrow=839,size_p=1.0,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_size_4 = run_RCD_size(folder_path,files,startrow=839,size_p=0.5,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)\n",
    "result_RCD_size_5 = run_RCD_size(folder_path,files,startrow=839,size_p=0.20,nodes=nodes,edges_list=edges,key_nodes=check_nodes,colors=colors,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f429ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_1-1.2_1</th>\n",
       "      <th>feeder_3-1.3_2</th>\n",
       "      <th>gripper_1-1.4_3</th>\n",
       "      <th>max_Vel_2-1.5_4</th>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200%</th>\n",
       "      <td>[cam_1_X, cam_1_Y]</td>\n",
       "      <td>[EoL_5_X, EoL_6_X]</td>\n",
       "      <td>[EoL_3_Y, EoL_4_Y, EoL_4_X]</td>\n",
       "      <td>[rob_2_maxVel]</td>\n",
       "      <td>[cam_1_X, EoL_5_X]</td>\n",
       "      <td>[EoL_6_X, EoL_5_Y]</td>\n",
       "      <td>[EoL_4_X, EoL_6_Y, EoL_5_X]</td>\n",
       "      <td>[EoL_6_X]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150%</th>\n",
       "      <td>[cam_1_X, EoL_5_X]</td>\n",
       "      <td>[EoL_4_X, EoL_6_Y]</td>\n",
       "      <td>[EoL_3_Y, rob_1_supply]</td>\n",
       "      <td>[rob_2_maxVel, EoL_6_Y, EoL_6_X]</td>\n",
       "      <td>[EoL_3_X, EoL_3_Y]</td>\n",
       "      <td>[cam_3_Y, EoL_6_Y, EoL_3_Y]</td>\n",
       "      <td>[EoL_6_X, rob_1_supply]</td>\n",
       "      <td>[EoL_6_Y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>[EoL_3_X, cam_1_X]</td>\n",
       "      <td>[EoL_5_X, EoL_4_Y]</td>\n",
       "      <td>[EoL_4_X, EoL_5_X]</td>\n",
       "      <td>[EoL_6_Y, EoL_6_X]</td>\n",
       "      <td>[EoL_3_Y, EoL_4_X]</td>\n",
       "      <td>[EoL_6_X, EoL_4_Y, EoL_3_Y]</td>\n",
       "      <td>[EoL_3_Y, EoL_6_Y]</td>\n",
       "      <td>[EoL_6_X, EoL_6_Y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>[EoL_1_Y, EoL_3_Y]</td>\n",
       "      <td>[EoL_5_Y, EoL_2_Y]</td>\n",
       "      <td>[EoL_3_Y, EoL_6_X]</td>\n",
       "      <td>[EoL_6_Y, EoL_3_Y, EoL_6_X]</td>\n",
       "      <td>[EoL_3_Y, EoL_1_Y]</td>\n",
       "      <td>[EoL_1_Y, EoL_4_X, EoL_5_X]</td>\n",
       "      <td>[EoL_6_X, EoL_5_X]</td>\n",
       "      <td>[EoL_6_X, EoL_1_Y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>[EoL_2_Y, EoL_3_X]</td>\n",
       "      <td>[cam_3_Y, EoL_2_X]</td>\n",
       "      <td>[cam_2_X, EoL_5_X, EoL_5_Y]</td>\n",
       "      <td>[EoL_1_Y, EoL_5_X]</td>\n",
       "      <td>[EoL_2_Y, EoL_6_X]</td>\n",
       "      <td>[cam_3_Y, EoL_2_X, EoL_1_Y]</td>\n",
       "      <td>[EoL_2_X, rob_1_supply]</td>\n",
       "      <td>[EoL_6_Y, rob_2_maxVel, cam_2_X]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 size_1-1.2_1      feeder_3-1.3_2  \\\n",
       "Data Size                                           \n",
       "200%       [cam_1_X, cam_1_Y]  [EoL_5_X, EoL_6_X]   \n",
       "150%       [cam_1_X, EoL_5_X]  [EoL_4_X, EoL_6_Y]   \n",
       "100%       [EoL_3_X, cam_1_X]  [EoL_5_X, EoL_4_Y]   \n",
       "50%        [EoL_1_Y, EoL_3_Y]  [EoL_5_Y, EoL_2_Y]   \n",
       "20%        [EoL_2_Y, EoL_3_X]  [cam_3_Y, EoL_2_X]   \n",
       "\n",
       "                       gripper_1-1.4_3                   max_Vel_2-1.5_4  \\\n",
       "Data Size                                                                  \n",
       "200%       [EoL_3_Y, EoL_4_Y, EoL_4_X]                    [rob_2_maxVel]   \n",
       "150%           [EoL_3_Y, rob_1_supply]  [rob_2_maxVel, EoL_6_Y, EoL_6_X]   \n",
       "100%                [EoL_4_X, EoL_5_X]                [EoL_6_Y, EoL_6_X]   \n",
       "50%                 [EoL_3_Y, EoL_6_X]       [EoL_6_Y, EoL_3_Y, EoL_6_X]   \n",
       "20%        [cam_2_X, EoL_5_X, EoL_5_Y]                [EoL_1_Y, EoL_5_X]   \n",
       "\n",
       "                 size_1-1.6_5               feeder_3-1.7_6  \\\n",
       "Data Size                                                    \n",
       "200%       [cam_1_X, EoL_5_X]           [EoL_6_X, EoL_5_Y]   \n",
       "150%       [EoL_3_X, EoL_3_Y]  [cam_3_Y, EoL_6_Y, EoL_3_Y]   \n",
       "100%       [EoL_3_Y, EoL_4_X]  [EoL_6_X, EoL_4_Y, EoL_3_Y]   \n",
       "50%        [EoL_3_Y, EoL_1_Y]  [EoL_1_Y, EoL_4_X, EoL_5_X]   \n",
       "20%        [EoL_2_Y, EoL_6_X]  [cam_3_Y, EoL_2_X, EoL_1_Y]   \n",
       "\n",
       "                       gripper_1-1.8_7                   max_Vel_2-1.9_8  \n",
       "Data Size                                                                 \n",
       "200%       [EoL_4_X, EoL_6_Y, EoL_5_X]                         [EoL_6_X]  \n",
       "150%           [EoL_6_X, rob_1_supply]                         [EoL_6_Y]  \n",
       "100%                [EoL_3_Y, EoL_6_Y]                [EoL_6_X, EoL_6_Y]  \n",
       "50%                 [EoL_6_X, EoL_5_X]                [EoL_6_X, EoL_1_Y]  \n",
       "20%            [EoL_2_X, rob_1_supply]  [EoL_6_Y, rob_2_maxVel, cam_2_X]  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_size_RCD = pd.DataFrame()\n",
    "result_size_RCD = pd.concat([result_RCD_size_1,result_RCD_size_2,result_RCD_size_3,result_RCD_size_4,result_RCD_size_5], ignore_index=True)\n",
    "result_size_RCD['Data Size'] = pd.DataFrame({'RCD':['200%','150%','100%','50%','20%']})\n",
    "result_size_RCD.set_index('Data Size', inplace=True)\n",
    "result_size_RCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85500f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200%</th>\n",
       "      <th>150%</th>\n",
       "      <th>100%</th>\n",
       "      <th>50%</th>\n",
       "      <th>20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  200%   150%   100%  50%  20%\n",
       "size_1-1.6_5     1.000  0.000  0.000  0.0  0.0\n",
       "feeder_3-1.7_6   0.000  1.000  0.000  0.0  1.0\n",
       "gripper_1-1.8_7  0.000  1.000  0.000  0.0  1.0\n",
       "max_Vel_2-1.9_8  0.000  0.000  0.000  0.0  1.0\n",
       "Total            0.375  0.625  0.125  0.0  0.5"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_3top = results_top_3(result_size_RCD,abnormal_sets)\n",
    "result_data_3top.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45198971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200%</th>\n",
       "      <th>150%</th>\n",
       "      <th>100%</th>\n",
       "      <th>50%</th>\n",
       "      <th>20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size_1-1.6_5</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeder_3-1.7_6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gripper_1-1.8_7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_Vel_2-1.9_8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  200%   150%  100%  50%   20%\n",
       "size_1-1.6_5     1.000  0.000   0.0  0.0  0.00\n",
       "feeder_3-1.7_6   0.000  1.000   0.0  0.0  1.00\n",
       "gripper_1-1.8_7  0.000  0.000   0.0  0.0  0.00\n",
       "max_Vel_2-1.9_8  0.000  0.000   0.0  0.0  0.00\n",
       "Total            0.375  0.375   0.0  0.0  0.25"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_1top = results_top_1(result_size_RCD,abnormal_sets)\n",
    "result_data_1top.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
